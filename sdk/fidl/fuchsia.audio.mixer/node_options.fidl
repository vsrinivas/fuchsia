// Copyright 2022 The Fuchsia Authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

library fuchsia.audio.mixer;

using fuchsia.audio;
using fuchsia.audio.effects;
using fuchsia.media2;
using fuchsia.hardware.audio;
using zx;

/// A Producer node can pull from a StreamSink or a RingBuffer.
type ProducerDataSource = flexible resource union {
    1: stream_sink StreamSinkProducer;
    2: ring_buffer fuchsia.audio.RingBuffer;
};

/// A Consumer node can write to a StreamSink or a RingBuffer.
type ConsumerDataSource = flexible resource union {
    1: stream_sink StreamSinkConsumer;
    2: ring_buffer fuchsia.audio.RingBuffer;
};

/// A producer driven by a [`fuchsia.media2/StreamSink`].
type StreamSinkProducer = resource table {
    /// Channel to receive packets.
    ///
    /// Required.
    1: server_end server_end:fuchsia.media2.StreamSink;

    /// Encoding of this audio stream.
    ///
    /// Required.
    2: format fuchsia.audio.Format;

    /// Clock for this node's destination streams.
    ///
    /// Required.
    3: reference_clock ReferenceClock;

    /// Payload buffer used by packets on this StreamSink.
    ///
    /// Required.
    4: payload_buffer zx.handle:VMO;

    /// Packet timestamps use "media time", which has
    /// `media_ticks_per_second_numerator` ticks every
    /// `media_ticks_per_second_denominator` seconds.
    5: media_ticks_per_second_numerator uint64;
    6: media_ticks_per_second_denominator uint64;
};

/// A consumer driven by a [`fuchsia.media2/StreamSink`].
type StreamSinkConsumer = resource table {
    /// Channel to send packets.
    ///
    /// Required.
    1: client_end client_end:fuchsia.media2.StreamSink;

    /// Encoding of this audio stream.
    ///
    /// Required.
    2: format fuchsia.audio.Format;

    /// Clock for this node's source stream.
    ///
    /// Required.
    3: reference_clock ReferenceClock;

    /// Payload buffer used by packets on this StreamSink.
    ///
    /// Required.
    4: payload_buffer zx.handle:VMO;

    /// Packet timestamps use "media time", which has
    /// `media_ticks_per_second_numerator` ticks every
    /// `media_ticks_per_second_denominator` seconds.
    5: media_ticks_per_second_numerator uint64;
    6: media_ticks_per_second_denominator uint64;

    /// Numer of frames to produce in each packet.
    ///
    /// Optional.
    7: frames_per_packet uint32;
};

/// A handle to a reference clock.
type ReferenceClock = resource table {
    /// Name of this clock, used for diagnostics only. See "IDs and names" in the
    /// comments for [`Graph`].
    ///
    /// Optional. If not specified, a default name is derived from whatever node
    /// is using this clock.
    1: name string:MAX_NAME_LENGTH;

    /// The clock must be ZX_CLOCK_OPT_MONOTONIC and ZX_CLOCK_OPT_CONTINUOUS
    /// with ZX_RIGHT_READ. See "Clocks" under the description for [`Graph`].
    ///
    /// Required.
    2: handle zx.handle:CLOCK;

    /// Domain of the clock, if known.
    ///
    /// Optional. If not specified, defaults to CLOCK_DOMAIN_EXTERNAL.
    3: domain fuchsia.hardware.audio.clock_domain;
};

/// Return value for CreateCustom.
type CustomNodeProperties = resource table {
    /// IDs for each source node. See comments at [`Graph.CreateCustom`].
    ///
    /// Required.
    1: source_ids vector<NodeId>:fuchsia.audio.effects.MAX_INPUT_STREAMS;

    /// IDs for each destination node. See comments at [`Graph.CreateCustom`].
    ///
    /// Required.
    2: dest_ids vector<NodeId>:fuchsia.audio.effects.MAX_OUTPUT_STREAMS;
};

/// Every node is part of an output pipeline or an input pipeline.
///
/// Output pipelines, also known as "render" pipelines, produce audio that is
/// written to an output device, usually some kind of speaker. Output pipelines
/// operate on audio that should be rendered to an output device at some time
/// in the future.
///
/// Input pipelines, also known as "capture" pipelines, consume audio that is
/// read from an input device, usually some kind of microphone. Input pipelines
/// operate on audio that was captured from an input device at some time in the
/// past.
///
/// Input pipelines can read from output pipelines. This is commonly known as a
/// "loopback" capture.
///
/// Input pipelines cannot write to output pipelines. This is not allowed
/// because it creates a timestamp paradox: When an input pipeline reads a
/// frame, it assigns the frame timestamp `T`, which represents the time at
/// which the frame was captured from an input device. Timestamp `T` occurred in
/// the past. When we forward that frame to an output pipeline, the frame will
/// underflow because time `T` is in the past. It can sometimes be useful to
/// connect pipelines in this way, but this must be done by the client, who is
/// responsible for re-timstamping captured frames in an appropriate way before
/// forwarding those frames to an output pipeline.
type PipelineDirection = strict enum {
    OUTPUT = 1;
    INPUT = 2;
};
