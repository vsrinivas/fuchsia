// Copyright 2016 The Fuchsia Authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

library fuchsia.media;

// A special value (expressed in decibels) which expressly silences an outgoing
// audio stream, regardless of other stream|device|environment characteristics.
// Note: this gain value is entirely separate from the stream's mute state.
const float32 kMutedGain = -160.0;

// An outgoing audio stream's maximum permitted above-unity gain, in decibels.
const float32 kMaxGain = 24.0;

struct AudioPacket {
  // Time at which the audio is to be presented, or was captured, as measured on
  // the reference clock.
  int64 timestamp = kNoTimestamp;

  // Offset of the packet payload in the indicated shared buffer.
  uint64 payload_offset;

  // Size in bytes of the packet payload.
  uint64 payload_size;

  // A collection of flags (see constants above) describing properties of this
  // packet.
  uint32 flags = 0;
};

interface AudioRendererGainControl {
  //////////////////////////////////////////////////////////////////////////////
  //
  // Methods and constants related to gain control.
  //
  // Audio renderers permit the application control of gain (expressed in
  // decibels) along with an independent, stateful, mute control.  The initial
  // default settings are 0.0dB and mute=false.
  //
  //////////////////////////////////////////////////////////////////////////////

  2: SetGainMute(float32 gain,
                 bool mute,             // false
                 uint32 flags)          // kGainFlagGainValid
    -> (float32 gain, bool mute);
  3: SetGainMuteNoReply(float32 gain,
                        bool mute,      // false
                        uint32 flags);  // kGainFlagGainValid
};

const uint32 kGainFlagGainValid = 0x01;
const uint32 kGainFlagMuteValid = 0x02;

interface AudioRenderer2 {
  //////////////////////////////////////////////////////////////////////////////
  //
  // Methods related to interface configuration.
  //
  // Audio renderers can be in one of two states at any point in time, either
  // the configurable state or the operational state.  A renderer is considered
  // to be operational any time it has packets queued and waiting to be
  // rendered.  configurable state.  When a renderer has entered the operational
  // state of its life, any attempt call a config method in the interface is
  // considered to be illegal and will result in termination of the interface's
  // connection to the audio server.
  //
  // If an audio renderer needs to be reconfigured, it is best practice to
  // always flush the renderer before starting to reconfigure it.
  //
  //////////////////////////////////////////////////////////////////////////////

  // Sets the PCM type of the stream to be delivered by the client.
  101: SetPcmStreamType(AudioStreamType stream_type);

  // TODO(dalesat): For compressed pass-through, add another SetStreamType-ish
  // method that allows encoding to be specified.

  // Set the shared buffer used to transport the frames of audio to be rendered.
  //
  // TODO(johngro) : consider extending this so that multiple payload buffers
  // may be assigned if needed.
  102: SetPayloadBuffer(handle<vmo> payload_buffer);

  // Sets the units used by the media/pts timeline.  By default, these units are
  // nanoseconds (1e9/1).  If you are using timestamps from an MPEG-PS
  // container, you would change this to 90000/1 (90 KHz).
  103: SetPtsUnits(uint32 tick_per_second_numerator,
                   uint32 tick_per_second_denominator);

  // Sets the maximum distance (in frames) between an explicit PTS (provided by
  // the user) and the expected PTS (determined using interpolation) which may
  // occur in an audio packet stream, and still be treated as 'continuous' by
  // the audio renderer.
  //
  // Defaults to RoundUp((AudioFPS/PTSTicksPerSec) / 2.0) / AudioFPS
  // The majority of uses should not need to change this value from its
  // default.
  //
  // Example:
  // A user is playing back 48KHz audio from a container, which also contains
  // video and needs to be synchronized with the audio.  The timestamps are
  // provided explicitly per packet by the container, and expressed in mSec
  // units.  This means that a single tick of the media timeline (1 mSec)
  // represents exactly 48 frames of audio.  The application in this scenario
  // delivers packets of audio to the renderer, each with exactly 470 frames of
  // audio, and each with an explicit timestamp set to the best possible
  // representation of the presentation time (given this media clock's
  // resolution).  So, starting from zero, the timestamps would be..
  //
  // [ 0, 10, 20, 29, 39, 49, 59, 69, 78, 88, ... ]
  //
  // In this example, attempting to use the presentation time to compute the
  // starting frame number of the audio in the packet would be wrong the
  // majority of the time.  The first timestamp is correct (by definition), but
  // it will be 24 packets before the timestamps and frame numbers come back
  // into alignment (the 24th packet would start with the 11280th audio frame
  // and have a PTS of exactly 235).
  //
  // One way to fix this situation is to set the PTS continuity threshold
  // (henceforth, CT) for the stream to be equal to 1/2 of the time taken by the
  // number of frames contained within a single tick of the media clock, rounded
  // up.  In this scenario, that would be 24.0 frames of audio, or 500 uSec.
  // Any packets whose expected PTS was within +/-CT frames of the explicitly
  // provided PTS would be considered to be a continuation of the previous frame
  // of audio.
  //
  // Other possible uses:
  // Users who are scheduling audio explicitly relative to a clock which has not
  // been configured as the reference clock can use this value to control the
  // maximum acceptable synchronization error before a discontinuity is
  // introduced.  Eg, if a user is scheduling audio based on a recovered common
  // media clock, and has not published that clock as the reference clock, and
  // they set the CT to 20mSec, then up to 20mSec of drift error can accumulate
  // before the audio renderer deliberately inserts a presentation discontinuity
  // to account for the error.
  //
  // Users whose need to deal with a container where their timestamps may be
  // even less correct than +/- 1/2 of a PTS tick may set this value to
  // something larger.  This should be the maximum level of inaccuracy present
  // in the container timestamps, if known.  Failing that, it could be set to
  // the maximum tolerable level of drift error before the timestamps are
  // explicitly obeyed.  Finally, a user could set this number to a very large
  // value (86400.0 seconds, for example) to effectively cause *all* timestamps
  // to be ignored after the first, and have all the audio be treated as
  // continuous with any previously delivered packets.  Conversely, users who
  // wish to *always* explicitly schedule their audio exactly may specify a CT
  // of 0.
  //
  104: SetPtsContinuityThreshold(float32 threshold_seconds);

  // Set the reference clock used to control playback rate and as the reference
  // for scheduled clock transformations.
  //
  // TODO(johngro) : refine this data type when we have a solid definition of
  // what a clock handle is/looks like.  Also, should we allow the user to lock
  // their rate to CLOCK_MONO instead of following the default (perhaps dynamic)
  // system rate?
  105: SetReferenceClock(handle ref_clock);

  //////////////////////////////////////////////////////////////////////////////
  //
  // Methods related to transport and buffering of media payloads.
  //
  //////////////////////////////////////////////////////////////////////////////

  // Send a packet of audio to be rendered.  The callback (if requested) will be
  // called either when the media has been completely rendered, or when the
  // packet has been flushed.
  //
  // TODO(johngro): Collapse these if we ever have optional retvals in FIDL
  201: SendPacket(AudioPacket packet) -> ();
  202: SendPacketNoReply(AudioPacket packet);

  // Immediately flush all media in the pipeline which is currently waiting to
  // be rendered.  Specifically this means that...
  //
  // 1) Any packets which are still in the queue waiting to be rendered will be
  //    released and have their SendPacket callback (if any) invoked.
  // 2) Any PTS interpolation related state will be reset.
  // 3) Any clock control state (such as where in media time to resume after a
  //    pause event) will be reset.
  // 4) Finally, the Flush callback (if requested) will be invoked.
  //
  // TODO(johngro): Collapse these if we ever have optional retvals in FIDL
  203: Flush() -> ();
  204: FlushNoReply();

  //////////////////////////////////////////////////////////////////////////////
  //
  // Methods related to media playback control.
  //
  // TODO(johngro) : Is it OK to restrict users to play/pause, or should we
  // allow them to specify an arbitrary playback ratio?
  //
  // TODO(johngro) : This API definition removes the ability to schedule
  // timeline transformations at specific points (either in real time, or in
  // media time).  While this is probably completely fine for the majority of
  // users, this could be an issue for users who wish to implement distributed
  // synchronized media players, and users who may wish to implement local
  // splicing by using multiple players which can start/stop at exact
  // coordinated points in time.  We should consider whether or not the ability
  // to schedule transformations at specific times is valuable enough to add to
  // the API or not.
  //
  //////////////////////////////////////////////////////////////////////////////

  // Immediately put the renderer into the playing state, and then report the
  // relationship between the media and reference timelines which was
  // established (if requested).
  //
  // The act of placing a renderer into the playback state establishes a
  // relationship between the user-defined media (or presentation) timeline, and
  // the real-world reference timeline.  In order to do so, two timestamps (one
  // on the media timeline, one on the reference timeline) must be established
  // as being related.  IOW - The audio frame at timestamp X will be presented
  // at reference time Y.  Users may explicitly provide both timestamps, one of
  // the timestamps, or neither of the timestamps, depending on their use case.
  // A timestamp may be omitted by supplying the special value kNoTimestamp.
  // In the case that a timestamp is omitted, the AudioRenderer automatically
  // deduce the timestamp to use based on the following rules.
  //
  // Reference Time
  // If the reference time is omitted, the audio renderer will select a
  // reference time to begin presentation which is currently safe based on the
  // minimum lead times for each of the audio outputs the renderer is currently
  // bound to.  For example, if the renderer was bound to an internal audio
  // output which required a minimum of 3 mSec of lead time, and an HDMI output
  // which required a minimum of 75 mSec of lead time, the renderer might
  // automatically schedule presentation to begin 80 mSec from now.
  //
  // Media Time
  // If the media time is omitted, the audio renderer will select one of two
  // value to use.  If the renderer is resuming from the paused state, and has
  // not been flushed since being paused, then the renderer will use the time,
  // in the media timeline, at which the presentation became paused as the
  // media_time value.  If the renderer is being placed in the playing state for
  // the first time following either startup or a flush operation, the
  // media_time selected will be the PTS of the first payload in the pending
  // queue.  If the pending queue is empty, a value of zero will be used for the
  // media_time.
  //
  // Return Value
  // When requested, the renderer will return the reference_time and media_time
  // which were selected and used (whether they were explicitly specified or
  // not) in the return value of the play call.
  //
  // Examples
  // A user has queued some audio using SendPacket and simply wishes them to
  // start playing as soon as possible.  The user may simply call Play(), with
  // no explicit timestamps provided.
  //
  // A user has queued some audio using SendPacket, and wishes to start playback
  // at reference time X, in sync with another stream (either audio, or video),
  // either initially or after flushing.  The user would call Play(X).
  //
  // A user has queued some audio using SendPacket.  The first of these packets
  // has a PTS of zero, and the user wishes to begin playback as soon as
  // possible, but wishes to skip the first X media timeline units worth of
  // audio.  The user would call Play(kNoTimestamp, X).
  //
  // A user has queued some audio using SendPacket and is attempting to present
  // media in synch with another player in a different device.  The coordinator
  // of the group of distributed players sends an explicit message to each
  // player telling them to begin presentation of media_time X at the group's
  // shared reference time Y.  The user would call Play(Y, X).
  //
  // TODO(johngro): Define behavior in the case that a user calls Play while the
  // system is already in the playing state.  We should probably do nothing but
  // provide a valid correspondence pair in response unless both the reference
  // and the media time are provided, in which case we should introduce a
  // discontinuity.
  //
  // TODO(johngro): Collapse these if we ever have optional retvals in FIDL
  301: Play(int64 reference_time,         // kNoTimestamp
            int64 media_time)             // kNoTimestamp
    -> (int64 reference_time, int64 media_time);
  302: PlayNoReply(int64 reference_time,  // kNoTimestamp
                   int64 media_time);     // kNoTimestamp

  // Immediately put the renderer into the paused state and then report the
  // relationship between the media and reference timelines which was
  // established (if requested).
  //
  // If omitted, the media time to be paused at will be determined by the
  // current relationship between the media timeline and its reference timeline,
  // as determined by the previously delivered play command.
  //
  // TODO(johngro): Define behavior in the case that a user calls Pause while
  // the system is already in the paused state.  We should probably do nothing
  // but provide a valid correspondence pair in response unless an explicit
  // media time is provided, in which case we should redefine the resume time,
  // introducing a discontinuity in the process.
  //
  // TODO(johngro): Collapse these if we ever have optional retvals in FIDL
  303: Pause() -> (int64 reference_time, int64 media_time);
  304: PauseNoReply();

  // Immediately set the gain/mute of an audio renderer gain stage to the values
  // specified, subject to the gain/mute validity flags.  By default (eg, with
  // the default flags) without altering the mute status of the stream.  Reports
  // the gain/mute settings of the stage after the requested changes have been
  // applied.  Note: This means that the following stubs could be generated by a
  // client side library, if they were convenient for a user.
  //
  // SetGain(db) --> SetGainMuteNoReply(db, false, kGainFlagGainValid)
  // SetMute(mute) --> SetGainMuteNoReply(0.0, mute, kGainFlagMuteValid)
  // SetGainAndMute(db, mute) -->
  //    SetGainMuteNoReply(db, mute, kGainFlagGainValid | kGainFlagMuteValid)
  // GetGainMute() --> SetGainMute(0, false, 0)
  //
  // TODO(johngro): Collapse these if we ever have optional retvals in FIDL
  401: SetGainMute(float32 gain,
                   bool mute,             // false
                   uint32 flags)          // kGainFlagGainValid
    -> (float32 gain, bool mute);
  402: SetGainMuteNoReply(float32 gain,
                          bool mute,      // false
                          uint32 flags);  // kGainFlagGainValid

  // Create a new interface (bound to this renderer) which is restricted to gain
  // control functions only.
  500: DuplicateGainControlInterface(request<AudioRendererGainControl> @request);

  // Enable or disable notifications about changes to the minimim clock lead
  // time (in nanoseconds) for this audio renderer.  Calling this method with
  // enabled set to true will trigger an immediate OnMinLeadTimeChanged event
  // with the current minimum lead time for the renderer.  If the value changes,
  // an OnMinLeadTimeChanged event will be raise with the new value.  This
  // behavior will continue until the user calls Enable with enabled set to
  // false.
  //
  // TODO(johngro): Should Enable have an optional -> () return value for
  // synchronization purposes?  Probably not; users should be able to simply
  // send a disable request and clear their event handler if they no longer care
  // to receive notifications.  Their in-process dispatcher framework can handle
  // draining and dropping any lead time changed events that were already in
  // flight when the disable message was sent.
  //
  // The minimum clock lead time is the amount of time ahead of RefClock.Now()
  // packets needs to arrive ahead of the playback clock transformation in order
  // for the mixer to be able to mix packet.  For example...
  //
  // ++ Let the PTS of packet X be P(X)
  // ++ Let the function which transforms PTS -> RefClock be R(p) (this function
  //    is determined by the call to Play(...)
  // ++ Let the minimum lead time be MLT
  //
  // If R(P(X)) < RefClock.Now() + MLT
  // Then the packet is late, and some (or all) of the packet's payload will
  // need to be skipped in order to present the packet at the scheduled time.
  //
  // TODO(johngro): What should the units be here?  Options include...
  //
  // 1) Normalized to nanoseconds (this is the current API)
  // 2) Reference clock units (what happens if the reference clock changes?)
  // 3) PTS units (what happens when the user changes the PTS units?)
  //
  601: EnableMinLeadTimeEvents(bool enabled);
  602: -> OnMinLeadTimeChanged(int64 min_lead_time_nsec);

  // TODO(johngro): Get rid of this method when possible.  Right now, it is used
  // by code which demands to use a synchronous FIDL interface to talk to
  // renderers.
  699: GetMinLeadTime() -> (int64 min_lead_time_nsec);

  // TODO(johngro): Spec methods/events which can be used for unintentional
  // discontinuity/underflow detection.
  //
  // TODO(johngro): Spec methods/events which can be used to report routing
  // changes.  (Presuming that they belong at this level at all; they may belong
  // on some sort of policy object).
  //
  // TODO(johngro): Spec methods/events which can be used to report policy
  // induced gain/ducking changes.  (Presuming that they belong at this level at
  // all; they may belong on some sort of policy object).
};
