// Copyright 2019 The Fuchsia Authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#version 460

//
// SEGMENT TTCK
//

#extension GL_GOOGLE_include_directive        : require
#extension GL_ARB_gpu_shader_int64            : require
#extension GL_KHR_shader_subgroup_arithmetic  : require
#extension GL_KHR_shader_subgroup_ballot      : require
#extension GL_KHR_shader_subgroup_shuffle     : require

//
// NOTE THAT THE SEGMENT TTCK KERNEL IS ENTIRELY DEPENDENT ON BOTH THE
// LAYOUT OF THE TTCK KEY AND THE SLAB GEOMETRY.
//
// IF THE TTCK KEY IS ALTERED THEN THIS KERNEL WILL NEED TO BE UPDATED
//
// NOTE THAT WE ASSUME TTCK KEYS WILL ALWAYS BE 64-BIT
//
// TTCK (32-BIT COMPARE) v1:
//
//  0                                                           63
//  | PAYLOAD/TTSB/TTPB ID | PREFIX | ESCAPE | LAYER |  X  |  Y  |
//  +----------------------+--------+--------+-------+-----+-----+
//  |          30          |    1   |    1   |   18  |  7  |  7  |
//
//
// TTCK (32-BIT COMPARE) v2:
//
//  0                                                           63
//  | PAYLOAD/TTSB/TTPB ID | PREFIX | ESCAPE | LAYER |  X  |  Y  |
//  +----------------------+--------+--------+-------+-----+-----+
//  |          30          |    1   |    1   |   15  |  9  |  8  |
//
//
// TTCK (64-BIT COMPARE) -- achieves 4K x 4K with an 8x16 tile: ( DEFAULT )
//
//  0                                                           63
//  | PAYLOAD/TTSB/TTPB ID | PREFIX | ESCAPE | LAYER |  X  |  Y  |
//  +----------------------+--------+--------+-------+-----+-----+
//  |          27          |    1   |    1   |   18  |  9  |  8  |
//

//
//
//

#include "config.h"
#include "target_layouts.h"

//
//
//

#if   defined( SPN_DEVICE_NVIDIA_SM50 )

#include "hotsort/vk/targets/nvidia/sm_35/u64/hs_config.h"
#include "hotsort/vk/targets/nvidia/hs_glsl_macros.h"

#elif defined( SPN_DEVICE_AMD_GCN3 )

#include "hotsort/vk/targets/amd/gcn3/u64/hs_config.h"
#include "hotsort/vk/targets/amd/hs_glsl_macros.h"

#elif defined( SPN_DEVICE_INTEL_GEN8 )

#include "hotsort/vk/targets/intel/gen8/u64/hs_config.h"
#include "hotsort/vk/targets/intel/hs_glsl_macros.h"

#else // FIXME --

#include "hotsort/vk/targets/intel/gen8/u64/hs_config.h"
#include "hotsort/vk/targets/intel/hs_glsl_macros.h"

// #error "SPN_KERNEL_SEGMENT_TTCK_TARGET undefined!"

#endif

//
// specialization constants
//
// FIXME -- we don't know what subgroup size we're going to get with INTEL
//

layout(local_size_x = HS_SLAB_THREADS) in;

//
// main(buffer ulong vout[],
//      buffer uint  indices[],
//      buffer uint  counter)
//

//
// NOTE: THIS DESCRIPTOR MUST BE COMPATIBLE WITH 'SPN_DESC_TTRKS'
//

SPN_TARGET_GLSL_DECL_KERNEL_SEGMENT_TTCK();

//
//
//

#if HS_SLAB_HEIGHT > 32
#define HS_COL_FLAG_TYPE  uint64_t
#else
#define HS_COL_FLAG_TYPE  uint
#endif

//
//
//

#define HS_KEY_TYPE_MAX  HS_KEY_TYPE(-1)

//
//
//

#undef  HS_VOUT
#define HS_VOUT               ttcks

#undef  HS_VOUT_LOAD
#define HS_VOUT_LOAD(idx)     packUint2x32(HS_VOUT[idx])

#undef  HS_VOUT_STORE
#define HS_VOUT_STORE(idx,kv) HS_VOUT[idx] = unpackUint2x32(kv)

//
//
//

void main()
{
  HS_SUBGROUP_PREAMBLE(); // because we don't know the size of Intel subgroups

  const uint gmem_base  = (gl_GlobalInvocationID.x >> HS_SLAB_THREADS_LOG2) * HS_SLAB_KEYS;
  const uint gmem_idx   = gmem_base + HS_SUBGROUP_LANE_ID();
  const uint linear_idx = gmem_base + HS_SUBGROUP_LANE_ID() * HS_SLAB_HEIGHT;

  //
  // LOAD ALL THE ROWS
  //
#undef  HS_SLAB_ROW
#define HS_SLAB_ROW(row,prev)                                                   \
  const HS_KEY_TYPE r##row = HS_VOUT_LOAD(gmem_idx + prev * HS_SLAB_WIDTH);

  HS_SLAB_ROWS();

  //
  // LOAD LAST REGISTER FROM COLUMN TO LEFT
  //
  // shuffle up the last key from the column to the left
  //
  // note that column 0 is undefined
  //
  const uint       lane  = max(gl_SubgroupInvocationID,1) - 1;
  HS_KEY_TYPE      r0    = HS_SUBGROUP_SHUFFLE(HS_REG_LAST(r),lane);
  HS_COL_FLAG_TYPE diffs = 0;

  if ((gmem_base > 0) && (gl_SubgroupInvocationID == 0))
    {
      //
      // If this is the first key in any slab but the first then it
      // broadcast loads the last key in previous slab
      //
      // Note that we only need the high dword but not all compilers
      // are cooperating right now
      //
      r0 = HS_VOUT_LOAD(gmem_base - 1); // FIXME - we only need high dword (uvec2[1])
    }
  else if (gl_GlobalInvocationID.x == 0)
    {
      //
      // If this is the first lane and first slab then record a diff
      //
      r0    = r1;
      diffs = 1;
    }

  //
  // FIND ALL VALID KEYS IN SLAB
  //
  HS_COL_FLAG_TYPE valid = 0;

#undef  HS_SLAB_ROW
#define HS_SLAB_ROW(row,prev)                                   \
  valid |= (r##row != HS_KEY_TYPE_MAX) ? (1 << prev) : 0;

  HS_SLAB_ROWS();

  //
  // FIND ALL DIFFERENCES IN SLAB
  //
#if 1 // branchless
#define SPN_YX_NEQ(row,prev) \
  (min(1,((unpackUint2x32(r##row)[1] ^ unpackUint2x32(r##prev)[1]) & SPN_TTCK_HI_MASK_YX)) << prev)
#else
#define SPN_YX_NEQ(row,prev) \
  (((unpackUint2x32(r##row)[1] ^ unpackUint2x32(r##prev)[1]) & SPN_TTCK_HI_MASK_YX) != 0 ? 1 << prev : 0)
#endif

#undef  HS_SLAB_ROW
#define HS_SLAB_ROW(row,prev)                   \
  diffs |= SPN_YX_NEQ(row,prev);

  HS_SLAB_ROWS();

  //
  // SUM UP THE DIFFERENCES
  //
  const HS_COL_FLAG_TYPE valid_diffs = valid & diffs;
  const uint             count       = bitCount(valid_diffs);
  const uint             inclusive   = subgroupInclusiveAdd(count);
  const uint             exclusive   = inclusive - count;

  //
  // RESERVE SPACE IN THE INDICES ARRAY
  //
  uint next = 0;

  if (gl_SubgroupInvocationID == gl_SubgroupSize - 1)
    next = atomicAdd(offsets_count[0],inclusive); // FIXME -- need a symbolic offset

  // distribute base across subgroup
  next = exclusive + subgroupBroadcast(next,HS_SLAB_THREADS - 1); // GLSL: ii has to be a compile-time constant

  //
  // STORE THE INDICES
  //
  // FIXME -- note that this is *not* going to result in coalesced
  // stores but it probably doesn't matter.  If it is determined that
  // this impacts performanced then it's straightforward to accumulate
  // these keys in local memory.
  //
#undef  HS_SLAB_ROW
#define HS_SLAB_ROW(row,prev)                   \
  if ((valid_diffs & (1 << prev)) != 0)         \
    offsets[next++] = linear_idx + prev;

  HS_SLAB_ROWS();

  //
  // TRANSPOSE THE SLAB AND STORE IT
  //
  HS_TRANSPOSE_SLAB();
}

//
//
//
