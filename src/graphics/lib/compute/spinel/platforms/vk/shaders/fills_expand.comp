// Copyright 2019 The Fuchsia Authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#version 460

//
// KERNEL: FILLS EXPAND
//

#extension GL_GOOGLE_include_directive : require
#extension GL_KHR_shader_subgroup_basic : require
#extension GL_KHR_shader_subgroup_vote : require
#extension GL_KHR_shader_subgroup_ballot : require
#extension GL_KHR_shader_subgroup_arithmetic : require
#extension GL_KHR_shader_subgroup_shuffle : require

//
//
//

#include "spn_config.h"
#include "vk_layouts.h"

//
// Enable NVIDIA-specific extension
//

#if (defined(SPN_DEVICE_FILLS_EXPAND_ENABLE_SUBGROUP_PARTITION_NV) &&                              \
     defined(GL_NV_shader_subgroup_partitioned))

#extension GL_NV_shader_subgroup_partitioned : require

#endif

//
//
//

#include "partition.h"

//
//
//

layout(local_size_x = SPN_DEVICE_FILLS_EXPAND_WORKGROUP_SIZE) in;

//
//
//

SPN_VK_GLSL_DECL_KERNEL_FILLS_EXPAND();

//
// CONSTANTS
//

#define SPN_FILLS_EXPAND_SUBGROUP_SIZE (1 << SPN_DEVICE_FILLS_EXPAND_SUBGROUP_SIZE_LOG2)

#define SPN_FILLS_EXPAND_SUBGROUPS                                                                 \
  (SPN_DEVICE_FILLS_EXPAND_WORKGROUP_SIZE / SPN_FILLS_EXPAND_SUBGROUP_SIZE)

#define SPN_FILLS_EXPAND_SUBGROUP_MASK SPN_BITS_TO_MASK(SPN_DEVICE_FILLS_EXPAND_SUBGROUP_SIZE_LOG2)

//
// BLOCK EXPANSION SIZE
//

#define SPN_FILLS_EXPAND_BLOCK_EXPAND_SIZE                                                         \
  (SPN_BLOCK_POOL_BLOCK_DWORDS / SPN_FILLS_EXPAND_SUBGROUP_SIZE)

//
// SELECT AN EXPANDER
//

#if (SPN_FILLS_EXPAND_BLOCK_EXPAND_SIZE == 1)

#define SPN_FILLS_EXPAND_BLOCK_EXPAND() SPN_EXPAND_1()
#define SPN_FILLS_EXPAND_BLOCK_EXPAND_I_LAST 0

#elif (SPN_FILLS_EXPAND_BLOCK_EXPAND_SIZE == 2)

#define SPN_FILLS_EXPAND_BLOCK_EXPAND() SPN_EXPAND_2()
#define SPN_FILLS_EXPAND_BLOCK_EXPAND_I_LAST 1

#elif (SPN_FILLS_EXPAND_BLOCK_EXPAND_SIZE == 4)

#define SPN_FILLS_EXPAND_BLOCK_EXPAND() SPN_EXPAND_4()
#define SPN_FILLS_EXPAND_BLOCK_EXPAND_I_LAST 3

#elif (SPN_FILLS_EXPAND_BLOCK_EXPAND_SIZE == 8)

#define SPN_FILLS_EXPAND_BLOCK_EXPAND() SPN_EXPAND_8()
#define SPN_FILLS_EXPAND_BLOCK_EXPAND_I_LAST 7

#elif (SPN_FILLS_EXPAND_BLOCK_EXPAND_SIZE == 16)

#define SPN_FILLS_EXPAND_BLOCK_EXPAND() SPN_EXPAND_16()
#define SPN_FILLS_EXPAND_BLOCK_EXPAND_I_LAST 15

#elif (SPN_FILLS_EXPAND_BLOCK_EXPAND_SIZE == 32)

#define SPN_FILLS_EXPAND_BLOCK_EXPAND() SPN_EXPAND_32()
#define SPN_FILLS_EXPAND_BLOCK_EXPAND_I_LAST 31

#else

#error "SPN_FILLS_EXPAND_BLOCK_EXPAND_SIZE not supported!"

#endif

//
// RUN-TIME PREDICATES
//

#define SPN_FILLS_EXPAND_BROADCAST(E, O, I)                                                        \
  subgroupBroadcast(E, O - I * SPN_FILLS_EXPAND_SUBGROUP_SIZE)

//
// UTILITY
//

#define SPN_FILLS_EXPAND_I_TO_CMD_NODE_DWORD(I)                                                    \
  (I * SPN_FILLS_EXPAND_SUBGROUP_SIZE + gl_SubgroupInvocationID)

//
//
//

#define SPN_FILLS_SCAN_SUBGROUP_MASK SPN_BITS_TO_MASK(SPN_DEVICE_FILLS_SCAN_SUBGROUP_SIZE_LOG2)

//
// NOTE: fills_scan and fills_expand subgroup sizes must match
//

#if (SPN_DEVICE_FILLS_SCAN_SUBGROUP_SIZE_LOG2 != SPN_DEVICE_FILLS_EXPAND_SUBGROUP_SIZE_LOG2)
#error "(SPN_DEVICE_FILLS_SCAN_SUBGROUP_SIZE_LOG2 != SPN_DEVICE_FILLS_EXPAND_SUBGROUP_SIZE_LOG2)"
#endif

//
// Test: (SPN_BLOCK_ID_TAG_PATH_RAT_QUAD <= tag <= SPN_BLOCK_ID_TAG_PATH_RAT_CUBIC)
//

#define SPN_FILLS_EXPAND_TAG_IS_RAT(tag_)                                                          \
  (uint(tag_ - SPN_BLOCK_ID_TAG_PATH_RAT_QUAD) <=                                                  \
   SPN_BLOCK_ID_TAG_PATH_RAT_CUBIC - SPN_BLOCK_ID_TAG_PATH_RAT_QUAD)

//
// Unfortunately we're going to have to lean on shared memory to
// accumulate the running base index of each raster command.
//

shared uint rast_base_smem[SPN_RAST_TYPE_COUNT];

//
// Partition SPN_RAST_TYPE_XXX values
//
// Note that NVIDIA provides the PTX instruction "match" on sm_70
// devices.
//
// This instruction returns a ballot of all matching lanes in a
// subgroup.
//
// The GLSL instruction is available via the
// GL_NV_shader_subgroup_partitioned extension:
//
//   uvec4 subgroupPartitionNV()
//
// Pre-sm_70 NVIDIA devices emulate the instruction.
//
// On non-NVIDIA platforms, this primitive can also be emulated.
//
#if (defined(SPN_DEVICE_FILLS_EXPAND_ENABLE_SUBGROUP_PARTITION_NV) &&                              \
     defined(GL_NV_shader_subgroup_partitioned))
//
// NVIDIA native partition
//
#define SPN_FILLS_EXPAND_PARTITION(part_, v_) part_ = subgroupPartitionNV(v_)

#elif defined(SPN_DEVICE_FILLS_EXPAND_ENABLE_SUBGROUP_PARTITION_BALLOT)
//
// Ballot partition
//
#if (SPN_FILLS_EXPAND_SUBGROUP_SIZE <= 32)
#define SPN_FILLS_EXPAND_PARTITION_BALLOT_UVEC SPN_PARTITION_BALLOT_UVEC1
#elif (SPN_FILLS_EXPAND_SUBGROUP_SIZE <= 64)
#define SPN_FILLS_EXPAND_PARTITION_BALLOT_UVEC SPN_PARTITION_BALLOT_UVEC2
#else
#error "SPN_PARTITION_BALLOT_UVEC*() subgroup size not supported!"
#endif

#define SPN_FILLS_EXPAND_PARTITION(part_, v_)                                                      \
  SPN_PARTITION_BALLOT_INIT(SPN_FILLS_EXPAND_PARTITION_BALLOT_UVEC, part_, v_, 0, 0, 0)            \
  SPN_PARTITION_BALLOT_TEST(SPN_FILLS_EXPAND_PARTITION_BALLOT_UVEC, part_, v_, 1, 1, 1)            \
  SPN_PARTITION_BALLOT_TEST(SPN_FILLS_EXPAND_PARTITION_BALLOT_UVEC, part_, v_, 2, 2, 2)            \
  SPN_PARTITION_BALLOT_TEST(SPN_FILLS_EXPAND_PARTITION_BALLOT_UVEC, part_, v_, 3, 3, 31)

#else  // defined(SPN_DEVICE_FILLS_EXPAND_ENABLE_SUBGROUP_PARTITION_BROADCAST)
//
// Broadcast partition -- default partitioning strategy
//
#if (SPN_FILLS_EXPAND_SUBGROUP_SIZE == 4)
#define SPN_FILLS_EXPAND_PARTITION(part_, v_) SPN_PARTITION_BROADCAST_4(part_, v_)
#elif (SPN_FILLS_EXPAND_SUBGROUP_SIZE == 8)
#define SPN_FILLS_EXPAND_PARTITION(part_, v_) SPN_PARTITION_BROADCAST_8(part_, v_)
#elif (SPN_FILLS_EXPAND_SUBGROUP_SIZE == 16)
#define SPN_FILLS_EXPAND_PARTITION(part_, v_) SPN_PARTITION_BROADCAST_16(part_, v_)
#elif (SPN_FILLS_EXPAND_SUBGROUP_SIZE == 32)
#define SPN_FILLS_EXPAND_PARTITION(part_, v_) SPN_PARTITION_BROADCAST_32(part_, v_)
#elif (SPN_FILLS_EXPAND_SUBGROUP_SIZE == 64)
#define SPN_FILLS_EXPAND_PARTITION(part_, v_) SPN_PARTITION_BROADCAST_64(part_, v_)
#endif

#endif

//
//
//

void
main()
{
  //
  // Every subgroup loads the same command.
  //
#if (SPN_FILLS_EXPAND_SUBGROUPS == 1)

  SPN_SUBGROUP_UNIFORM
  const uint sid = gl_WorkGroupID.x;

#else

  SPN_SUBGROUP_UNIFORM
  const uint sid = gl_WorkGroupID.x * SPN_FILLS_EXPAND_SUBGROUPS + gl_SubgroupID;

  // If the workgroup is made up of multiple subgroups then exit if
  // the trailing subgroups have no work to do.
  if (sid >= cmd_span)
    return;

#endif

  //
  // Where are we in the ring?
  //
  uint cmd_idx = sid + cmd_head;

  if (cmd_idx >= cmd_size)
    {
      cmd_idx -= cmd_size;
    }

  //
  // Load the fill cmd from the ring.
  //
  // The cmd is not subgroup uniform and may be repeatedly updated by
  // each subgroup invocation.
  //
  uvec4 cmd = fill_cmds[cmd_idx];

  //
  // Load the exclusive scan add of total primitives.
  //
#if (SPN_FILLS_EXPAND_SUBGROUP_SIZE == 4)
  //
  // gl_SubgroupSize == 4
  //
  uvec2 prim_exc = { fill_scan_dispatch[0 + gl_SubgroupInvocationID][3],
                     fill_scan_dispatch[4 + gl_SubgroupInvocationID][3] };

#elif (SPN_FILLS_EXPAND_SUBGROUP_SIZE >= 8)
  //
  // gl_SubgroupSize >= 8
  //
  uint prim_exc = 0;

#if (SPN_FILLS_EXPAND_SUBGROUP_SIZE > 8)
  if (gl_SubgroupInvocationID < SPN_RAST_TYPE_COUNT)
#endif
    {
      prim_exc = fill_scan_dispatch[gl_SubgroupInvocationID][3];
    }

#else
#error "Unexpected subgroup size!"
#endif

  //
  // Load the command's path primitive prefix base.
  //
  SPN_SUBGROUP_UNIFORM const uint fsp_idx = sid + (sid & ~SPN_FILLS_EXPAND_SUBGROUP_MASK);

#if (SPN_FILLS_EXPAND_SUBGROUP_SIZE == 4)
  //
  // gl_SubgroupSize == 4
  //
  uvec2 prim_base = {
    fill_scan_prefix[fsp_idx + 0 * SPN_FILLS_EXPAND_SUBGROUP_SIZE][gl_SubgroupInvocationID],
    fill_scan_prefix[fsp_idx + 1 * SPN_FILLS_EXPAND_SUBGROUP_SIZE][gl_SubgroupInvocationID]
  };

#elif (SPN_FILLS_EXPAND_SUBGROUP_SIZE >= 8)
  //
  // gl_SubgroupSize >= 8
  //
  uint prim_base = 0;

#if (SPN_FILLS_EXPAND_SUBGROUP_SIZE > 8)
  if (gl_SubgroupInvocationID < SPN_RAST_TYPE_COUNT)
#endif
    {
      // FIXME(allanmac): use a ~mask and multiply
      const uint fsp_row = (gl_SubgroupInvocationID / 4) * SPN_FILLS_EXPAND_SUBGROUP_SIZE;
      const uint uv4_idx = (gl_SubgroupInvocationID & 3);

      prim_base = fill_scan_prefix[fsp_idx + fsp_row][uv4_idx];
    }

#else
#error "Unexpected subgroup size!"
#endif

  //
  // Convert the fill_cmd.path_h to a rast_cmd.node_id.
  //
  // Unfortunately, this is a subgroup-wide "broadcast read" of a
  // single dword and is fairly low intensity.
  //
  const uint                path_h  = SPN_CMD_FILL_GET_PATH_H(cmd);
  SPN_SUBGROUP_UNIFORM uint node_id = bp_host_map[path_h];

  SPN_CMD_RASTERIZE_SET_NODE_ID(cmd, node_id);

  //
  // Is the fill command's transform affine or projective?
  //
  SPN_SUBGROUP_UNIFORM const bool is_affine = SPN_CMD_FILL_IS_TRANSFORM_TYPE_AFFINE(cmd);

  //
  // Load entire head into registers.
  //
  const uint h_idx = node_id * SPN_BLOCK_POOL_SUBBLOCK_DWORDS + gl_SubgroupInvocationID;

#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L) uint h##I = bp_blocks[h_idx + I * SPN_FILLS_EXPAND_SUBGROUP_SIZE];

  SPN_FILLS_EXPAND_BLOCK_EXPAND();

  //
  // Pick out count.nodes from the header.
  //
  uint count_nodes;

#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  if (SPN_PATH_HEAD_ELEM_IN_RANGE(SPN_FILLS_EXPAND_SUBGROUP_SIZE, SPN_PATH_HEAD_OFFSET_NODES, I))  \
    {                                                                                              \
      count_nodes = SPN_FILLS_EXPAND_BROADCAST(h##I, SPN_PATH_HEAD_OFFSET_NODES, I);               \
    }

  SPN_FILLS_EXPAND_BLOCK_EXPAND();

  //
  // Invalidate header.
  //
#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  if (!SPN_PATH_HEAD_ENTIRELY_HEADER(SPN_FILLS_EXPAND_SUBGROUP_SIZE, I))                           \
    {                                                                                              \
      if (SPN_PATH_HEAD_PARTIALLY_HEADER(SPN_FILLS_EXPAND_SUBGROUP_SIZE, I))                       \
        {                                                                                          \
          if (SPN_PATH_HEAD_IS_HEADER(SPN_FILLS_EXPAND_SUBGROUP_SIZE, I))                          \
            {                                                                                      \
              h##I = SPN_TAGGED_BLOCK_ID_INVALID;                                                  \
            }                                                                                      \
        }                                                                                          \
    }

  SPN_FILLS_EXPAND_BLOCK_EXPAND();

  //
  // Save the "next" node id.
  //
  SPN_SUBGROUP_UNIFORM const uint head_tbid =
    subgroupBroadcast(SPN_GLSL_CONCAT(h, SPN_FILLS_EXPAND_BLOCK_EXPAND_I_LAST),
                      SPN_FILLS_EXPAND_SUBGROUP_SIZE - 1);

  node_id = SPN_TAGGED_BLOCK_ID_GET_ID(head_tbid);

  //
  // All we need are the tags.
  //
#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  if (!SPN_PATH_HEAD_ENTIRELY_HEADER(SPN_FILLS_EXPAND_SUBGROUP_SIZE, I))                           \
    {                                                                                              \
      h##I = SPN_TAGGED_BLOCK_ID_GET_TAG(h##I);                                                    \
    }

  SPN_FILLS_EXPAND_BLOCK_EXPAND();

  //
  // Map block id path tags to rasterization types:
  //
  //                AFFINE                TAG_PATH                 PROJECTIVE
  //       -----------------------    ------------------    ------------------------
  //   0:             0               TAG_PATH_LINE         SPN_RAST_TYPE_PROJ_LINE
  //   1:             0               TAG_PATH_QUAD         SPN_RAST_TYPE_PROJ_QUAD
  //   2:             0               TAG_PATH_CUBIC        SPN_RAST_TYPE_PROJ_CUBIC
  //   3:  SPN_RAST_TYPE_LINE         TAG_PATH_RAT_QUAD                0
  //   4:  SPN_RAST_TYPE_QUAD         TAG_PATH_RAT_CUBIC               0
  //   5:  SPN_RAST_TYPE_CUBIC                                         0
  //   6:  SPN_RAST_TYPE_RAT_QUAD                           SPN_RAST_TYPE_RAT_QUAD
  //   7:  SPN_RAST_TYPE_RAT_CUBIC                          SPN_RAST_TYPE_RAT_CUBIC
  //
#if 1  // NOTE(allanmac): projective shaders aren't implemented in this CL

#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  if (!SPN_PATH_HEAD_ENTIRELY_HEADER(SPN_FILLS_EXPAND_SUBGROUP_SIZE, I))                           \
    {                                                                                              \
      if (h##I <= SPN_BLOCK_ID_TAG_PATH_RAT_CUBIC)                                                 \
        h##I += SPN_RAST_TYPE_LINE;                                                                \
    }

      SPN_FILLS_EXPAND_BLOCK_EXPAND();

#else

  if (is_affine)
    {
#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  if (!SPN_PATH_HEAD_ENTIRELY_HEADER(SPN_FILLS_EXPAND_SUBGROUP_SIZE, I))                           \
    {                                                                                              \
      if (h##I <= SPN_BLOCK_ID_TAG_PATH_RAT_CUBIC)                                                 \
        h##I += SPN_RAST_TYPE_LINE;                                                                \
    }

      SPN_FILLS_EXPAND_BLOCK_EXPAND();
    }
  else  // is_projective
    {
#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  if (!SPN_PATH_HEAD_ENTIRELY_HEADER(SPN_FILLS_EXPAND_SUBGROUP_SIZE, I))                           \
    {                                                                                              \
      if (SPN_FILLS_EXPAND_TAG_IS_RAT(h##I))                                                       \
        h##I += (SPN_RAST_TYPE_RAT_QUAD - SPN_BLOCK_ID_TAG_PATH_RAT_QUAD);                         \
    }

      SPN_FILLS_EXPAND_BLOCK_EXPAND();
    }

#endif

  //
  // Compute the base index for raster commands expanded from this
  // fill command.
  //
#if (SPN_FILLS_EXPAND_SUBGROUP_SIZE == 4)
  //
  // gl_SubgroupSize == 4
  //
  rast_base_smem[0 + gl_SubgroupInvocationID] = prim_exc[0] + prim_base[0];
  rast_base_smem[4 + gl_SubgroupInvocationID] = prim_exc[1] + prim_base[1];

#elif (SPN_FILLS_EXPAND_SUBGROUP_SIZE >= 8)
    //
    // gl_SubgroupSize >= 8
    //
#if (SPN_FILLS_EXPAND_SUBGROUP_SIZE > 8)
  if (gl_SubgroupInvocationID < SPN_RAST_TYPE_COUNT)
#endif
    {
      rast_base_smem[gl_SubgroupInvocationID] = prim_exc + prim_base;
    }

#else
#error "Unexpected subgroup size!"
#endif

  //
  // Partition and scatter store each row.
  //
  // Early exit from a block if it can be reasoned that there are no
  // more valid tags.
  //
  // Note: testing >= TAG_PATH_NEXT will work for subgroup size 4
  // because a rational cubic occupies 10 segments and a block will
  // hold at least 4 segments.
  //
#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  if (!SPN_PATH_HEAD_ENTIRELY_HEADER(SPN_FILLS_EXPAND_SUBGROUP_SIZE, I))                           \
    {                                                                                              \
      const bool is_valid = (h##I < SPN_RAST_TYPE_COUNT);                                          \
                                                                                                   \
      /* Early exit? */                                                                            \
      if (subgroupAll(!is_valid))                                                                  \
        return;                                                                                    \
                                                                                                   \
      uvec4 part;                                                                                  \
                                                                                                   \
      SPN_FILLS_EXPAND_PARTITION(part, h##I);                                                      \
                                                                                                   \
      subgroupMemoryBarrierShared();                                                               \
                                                                                                   \
      if (is_valid)                                                                                \
        {                                                                                          \
          const uint node_dword = SPN_FILLS_EXPAND_I_TO_CMD_NODE_DWORD(I);                         \
                                                                                                   \
          SPN_CMD_RASTERIZE_SET_NODE_DWORD(cmd, node_dword);                                       \
                                                                                                   \
          const uint base = rast_base_smem[h##I];                                                  \
                                                                                                   \
          const uint part_red = subgroupBallotBitCount(part);                                      \
          const uint part_exc = subgroupBallotExclusiveBitCount(part);                             \
                                                                                                   \
          rast_base_smem[h##I]       = base + part_red;                                            \
          rast_cmds[base + part_exc] = cmd;                                                        \
        }                                                                                          \
    }

  SPN_FILLS_EXPAND_BLOCK_EXPAND();

  //
  // Are there more nodes?
  //
  while (count_nodes-- > 0)
    {
      //
      // Update node id in rast command.
      //
      SPN_CMD_RASTERIZE_SET_NODE_ID(cmd, node_id);

      //
      // Load entire node into registers.
      //
      const uint n_idx = node_id * SPN_BLOCK_POOL_SUBBLOCK_DWORDS + gl_SubgroupInvocationID;

#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L) uint n##I = bp_blocks[n_idx + I * SPN_FILLS_EXPAND_SUBGROUP_SIZE];

      SPN_FILLS_EXPAND_BLOCK_EXPAND();

      //
      // Save the "next" node id.
      //
      SPN_SUBGROUP_UNIFORM const uint node_tbid =
        subgroupBroadcast(SPN_GLSL_CONCAT(n, SPN_FILLS_EXPAND_BLOCK_EXPAND_I_LAST),
                          SPN_FILLS_EXPAND_SUBGROUP_SIZE - 1);

      node_id = SPN_TAGGED_BLOCK_ID_GET_ID(node_tbid);

      //
      // All we need are the tags.
      //
#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L) n##I = SPN_TAGGED_BLOCK_ID_GET_TAG(n##I);

      SPN_FILLS_EXPAND_BLOCK_EXPAND();

      //
      // Map block id path tags to rasterization types.
      //
#if 1  // NOTE(allanmac): projective shaders aren't implemented in this CL

#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  if (n##I <= SPN_BLOCK_ID_TAG_PATH_RAT_CUBIC)                                                     \
    n##I += SPN_RAST_TYPE_LINE;

      SPN_FILLS_EXPAND_BLOCK_EXPAND();

#else

      if (is_affine)
        {
#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  if (n##I <= SPN_BLOCK_ID_TAG_PATH_RAT_CUBIC)                                                     \
    n##I += SPN_RAST_TYPE_LINE;

          SPN_FILLS_EXPAND_BLOCK_EXPAND();
        }
      else  // is_projective
        {
#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  if (SPN_FILLS_EXPAND_TAG_IS_RAT(n##I))                                                           \
    n##I += (SPN_RAST_TYPE_RAT_QUAD - SPN_BLOCK_ID_TAG_PATH_RAT_QUAD);

          SPN_FILLS_EXPAND_BLOCK_EXPAND();
        }

#endif

      //
      // Partition and scatter store each row.
      //
#undef SPN_EXPAND_X
#define SPN_EXPAND_X(I, N, P, L)                                                                   \
  {                                                                                                \
    const bool is_valid = (n##I < SPN_RAST_TYPE_COUNT);                                            \
                                                                                                   \
    /* Early exit? */                                                                              \
    if (subgroupAll(!is_valid))                                                                    \
      return;                                                                                      \
                                                                                                   \
    uvec4 part;                                                                                    \
                                                                                                   \
    SPN_FILLS_EXPAND_PARTITION(part, n##I);                                                        \
                                                                                                   \
    subgroupMemoryBarrierShared();                                                                 \
                                                                                                   \
    if (is_valid)                                                                                  \
      {                                                                                            \
        const uint node_dword = SPN_FILLS_EXPAND_I_TO_CMD_NODE_DWORD(I);                           \
                                                                                                   \
        SPN_CMD_RASTERIZE_SET_NODE_DWORD(cmd, node_dword);                                         \
                                                                                                   \
        const uint base = rast_base_smem[n##I];                                                    \
                                                                                                   \
        const uint part_red = subgroupBallotBitCount(part);                                        \
        const uint part_exc = subgroupBallotExclusiveBitCount(part);                               \
                                                                                                   \
        rast_base_smem[n##I]       = base + part_red;                                              \
        rast_cmds[base + part_exc] = cmd;                                                          \
      }                                                                                            \
  }

      SPN_FILLS_EXPAND_BLOCK_EXPAND();
    }
}

//
//
//
