// Copyright 2019 The Fuchsia Authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#version 460

//
//
//

#extension GL_GOOGLE_include_directive : require
#extension GL_KHR_shader_subgroup_basic : require
#extension GL_KHR_shader_subgroup_arithmetic : require

//
//
//

#include "spn_config.h"
#include "vk_layouts.h"

//
// CONSTANTS
//

#define SPN_FILLS_DISPATCH_SUBGROUP_SIZE (1 << SPN_DEVICE_FILLS_DISPATCH_SUBGROUP_SIZE_LOG2)

//
// If the subgroup size is less than SPN_BLOCK_ID_TAG_PATH_COUNT then
// we use a different strategy.
//

#define SPN_FILLS_DISPATCH_SUBGROUP_SIZE_LT_TAG_PATH_COUNT                                         \
  (SPN_FILLS_DISPATCH_SUBGROUP_SIZE < SPN_BLOCK_ID_TAG_PATH_COUNT)

//
//
//

#if SPN_FILLS_DISPATCH_SUBGROUP_SIZE_LT_TAG_PATH_COUNT

#extension GL_KHR_shader_subgroup_ballot : require

#endif

//
// FILLS DISPATCH
//
// This fixes up the (5) path primitive counts so they can be used by
// vkCmdDispatchIndirect().
//
// It also computes the exclusive prefix sum of the counts so each
// rasterization workgroup type (lines, quads, etc.) knows where to
// begin the cmd_rast[] buffer.
//
// The sum is stored in the 4 element of each quad.
//

layout(local_size_x = SPN_FILLS_DISPATCH_SUBGROUP_SIZE) in;

//
// main(buffer uint prim_counts[sizeof(uvec4) * 5]);
//

SPN_VK_GLSL_DECL_KERNEL_FILLS_DISPATCH();

//
//
//

void
main()
{
#if SPN_FILLS_DISPATCH_SUBGROUP_SIZE_LT_TAG_PATH_COUNT
  //
  // SIMD4 -- ARM Bifrost and maybe very small future GPUs
  //
  uvec4 dispatch0 = uvec4(0, 1, 1, 0);
  uvec4 dispatch1 = uvec4(0, 1, 1, 0);

  //
  // load
  //
  dispatch0[0] = fill_scan_counts[gl_SubgroupInvocationID];

  if (SPN_FILLS_DISPATCH_SUBGROUP_SIZE + gl_SubgroupInvocationID < SPN_BLOCK_ID_TAG_PATH_COUNT)
    dispatch1[0] = fill_scan_counts[SPN_FILLS_DISPATCH_SUBGROUP_SIZE + gl_SubgroupInvocationID];

  //
  // exclusive scan-add of fill_scan_counts
  //
  dispatch0[3] = subgroupExclusiveAdd(dispatch0[0]);

  const uint dispatch0_total =
    subgroupBroadcast(dispatch0[3] + dispatch0[0], SPN_FILLS_DISPATCH_SUBGROUP_SIZE - 1);

  dispatch1[3] = subgroupExclusiveAdd(dispatch1[0]) + dispatch0_total;

  //
  // store
  //
  fill_scan_counts_uvec4[gl_SubgroupInvocationID] = dispatch0;

  if (SPN_FILLS_DISPATCH_SUBGROUP_SIZE + gl_SubgroupInvocationID < SPN_BLOCK_ID_TAG_PATH_COUNT)
    fill_scan_counts_uvec4[SPN_FILLS_DISPATCH_SUBGROUP_SIZE + gl_SubgroupInvocationID] = dispatch1;

#else
  //
  // SIMD8+ -- every other GPU I'm aware of...
  //
  uvec4 dispatch = uvec4(0, 1, 1, 0);

  // valid lane?
  const bool is_valid = gl_SubgroupInvocationID < SPN_BLOCK_ID_TAG_PATH_COUNT;

  // load
  if (is_valid)
    dispatch[0] = fill_scan_counts[gl_SubgroupInvocationID];

  // exclusive scan-add of commands by type
  dispatch[3] = subgroupExclusiveAdd(dispatch[0]);

  // store
  if (is_valid)
    fill_scan_dispatch[gl_SubgroupInvocationID] = dispatch;

    //
    // DEBUG -- DUMP DISPATCHES
    //
#if 0

  uint debug_base = 0;

  if (gl_SubgroupInvocationID == 0)
    debug_base = atomicAdd(bp_debug_count[0], SPN_BLOCK_ID_TAG_PATH_COUNT * 4);

  debug_base = subgroupBroadcast(debug_base, 0) + gl_SubgroupInvocationID * 4;

  if (is_valid)
    {
      bp_debug[debug_base + 0] = dispatch[0];
      bp_debug[debug_base + 1] = dispatch[1];
      bp_debug[debug_base + 2] = dispatch[2];
      bp_debug[debug_base + 3] = dispatch[3];
    }

#endif

#endif
}

//
//
//
