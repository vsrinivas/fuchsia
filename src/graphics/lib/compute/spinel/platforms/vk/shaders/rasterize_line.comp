// Copyright 2019 The Fuchsia Authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#version 460

//
// FIXME -- *ALL* pre-new-X TTSK keys could be squelched if we're
// willing to pay for a pre-pass that reads from back to front and:
//
//   - converts all TTRK keys to TTSK keys
//   - sets squelched TTSK keys to +2
//   - unsquelched keys are +1
//

#extension GL_GOOGLE_include_directive             : require
//#extension GL_EXT_control_flow_attributes          : require
#extension GL_KHR_shader_subgroup_basic            : require
#extension GL_KHR_shader_subgroup_ballot           : require
#extension GL_KHR_shader_subgroup_arithmetic       : require
//#extension GL_KHR_shader_subgroup_shuffle          : require
//#extension GL_KHR_shader_subgroup_shuffle_relative : require

//
// RASTERIZE KERNEL
//

#include "spn_config.h"
#include "spn_vk_layouts.h"

//
// LOCAL DEFINITIONS
//

#define SPN_KERNEL_RASTERIZE_SUBGROUPS  (SPN_KERNEL_RASTERIZE_WORKGROUP_SIZE / \
                                         SPN_KERNEL_RASTERIZE_SUBGROUP_SIZE)

#define SPN_RASTERIZE_SEGMENT(tb_id)    (SPN_TAGGED_BLOCK_ID_GET_ID(tb_id) * \
                                         SPN_BLOCK_POOL_SUBBLOCK_DWORDS    + \
                                         gl_SubgroupInvocationID)

//
//
//

layout(local_size_x = SPN_KERNEL_RASTERIZE_WORKGROUP_SIZE) in;

//
// rasterize(buffer uint   bp_atomics[2],
//           buffer uint   bp_ids[],
//           buffer uint   bp_blocks[],
//           buffer uint   ttrk_atomic[1],
//           buffer uvec2  ttrk_extent[],
//           buffer float8 transforms[]
//           buffer float4 clips[]
//           buffer uvec4  fill_cmds[]
//           uniform uint  bp_mask,
//           uniform uint  count);
//

SPN_VK_GLSL_DECL_KERNEL_RASTERIZE_LINE();

//
//
//

struct spn_rasterize_smem
{
  uint scratch[SPN_KERNEL_RASTERIZE_SUBGROUP_SIZE];
};

#if ( SPN_KERNEL_RASTERIZE_SUBGROUPS == 1 )

shared spn_rasterize_smem smem;

#define SPN_RASTERIZE_SMEM() smem

#else

shared spn_rasterize_smem smem[SPN_KERNEL_RASTERIZE_SUBGROUPS];

#define SPN_RASTERIZE_SMEM() smem[gl_SubgroupID]

#endif

//
//
//

#define SPN_PATH_NODE_DWORD_IS_LAST(n)  (((n) & SPN_BLOCK_POOL_BLOCK_DWORDS_MASK) == SPN_BLOCK_POOL_BLOCK_DWORDS_MASK)

void
spn_segment_next(SPN_SUBGROUP_UNIFORM inout uint tb_id_idx,
                 SPN_SUBGROUP_UNIFORM inout uint block_id)
{
  block_id += (SPN_KERNEL_PATHS_COPY_SUBGROUP_SIZE / SPN_BLOCK_POOL_SUBBLOCK_DWORDS);

  // was that the last path segment in the block?
  if ((block_id & SPN_BLOCK_POOL_SUBBLOCKS_PER_BLOCK_MASK) == 0)
    {
      // is this the last tagged block id in the node?
      if (SPN_PATH_NODE_DWORD_IS_LAST(++tb_id_idx))
        tb_id_idx = SPN_TAGGED_BLOCK_ID_GET_ID(bp_blocks[tb_id_idx]) * SPN_BLOCK_POOL_SUBBLOCK_DWORDS;

      block_id = SPN_TAGGED_BLOCK_ID_GET_ID(bp_blocks[tb_id_idx]);
    }
}

//
//
//

void main()
{
  //
  // This is a subgroup/warp-centric kernel.
  //
  // Which subgroup in the grid is this?
  //
#if (SPN_KERNEL_RASTERIZE_WORKGROUP_SIZE == SPN_KERNEL_RASTERIZE_SUBGROUP_SIZE)
  SPN_SUBGROUP_UNIFORM const uint sid = gl_WorkGroupID.x;
#else
  SPN_SUBGROUP_UNIFORM const uint sid = gl_WorkGroupID.x * SPN_KERNEL_RASTERIZE_SUBGROUPS + gl_SubgroupID;

  // FIXME -- this is dynamically determined and won't be a push
  // constant and can be found in prim.counts[3]
  if (sid >= count)
    return; // empty subgroup
#endif

  //
  // Each subgroup processes a single command
  //
  SPN_SUBGROUP_UNIFORM const uvec4 cmd = rast_cmds[sid];

  //
  // NOTE THAT THE HOST IS RESPONSIBLE FOR SCALING THE TRANSFORM BY:
  //
  //   [ SPN_SUBPIXEL_RESL_X_F32, SPN_SUBPIXEL_RESL_Y_F32, 1.0f ]
  //
  // Coordinates are scaled to subpixel resolution.  All that matters
  // is that continuity is maintained between end path element
  // endpoints.
  //
  // It's the responsibility of the host to ensure that the transforms
  // are properly scaled either via intitializing a transform stack
  // with the subpixel resolution scaled identity or scaling the
  // transform before its loaded by a rasterization grid.
  //
  SPN_SUBGROUP_UNIFORM uint tb_id_idx =
    SPN_CMD_RASTERIZE_GET_NODE_ID(cmd) * SPN_BLOCK_POOL_SUBBLOCK_DWORDS + SPN_CMD_RASTERIZE_GET_NODE_DWORD(cmd);
  SPN_SUBGROUP_UNIFORM uint block_id  = SPN_TAGGED_BLOCK_ID_GET_ID(bp_blocks[tb_id_idx]);

  //
  // FIXME -- this initial load can be made faster by loading multiple keys
  //

  //
  // the initial segment idx and segments-per-block constant determine
  // how many block ids will need to be loaded
  //
  vec2 a0;

  a0.x = uintBitsToFloat(bp_blocks[SPN_RASTERIZE_SEGMENT(block_id)]);
  spn_segment_next(tb_id_idx,block_id);
  a0.y = uintBitsToFloat(bp_blocks[SPN_RASTERIZE_SEGMENT(block_id)]);
  spn_segment_next(tb_id_idx,block_id);

  vec2 a1;

  a1.x = uintBitsToFloat(bp_blocks[SPN_RASTERIZE_SEGMENT(block_id)]);
  spn_segment_next(tb_id_idx,block_id);
  a1.y = uintBitsToFloat(bp_blocks[SPN_RASTERIZE_SEGMENT(block_id)]);;
  spn_segment_next(tb_id_idx,block_id);

  //
  // Apply transform
  //
  // Note that we only care if the end points are rounded to subpixel precision
  //
  // The affine transformation requires 8 FMA + 4 ROUND operations
  //
  //   A---------B[0]-+
  //   | sx  shx | tx |
  //   | shy sy  | ty |
  //   B[1]------D----+
  //   | w0  w1  | 1  |
  //   +---------+----+

  SPN_SUBGROUP_UNIFORM const uint t_idx = SPN_CMD_RASTERIZE_GET_TRANSFORM(cmd);

  SPN_SUBGROUP_UNIFORM const mat2 t_a   = mat2(fill_quads[t_idx  ]);
  SPN_SUBGROUP_UNIFORM const mat2 t_b   = mat2(fill_quads[t_idx+1]);

  vec2 xy0 = t_a * a0 + t_b[0];
  vec2 xy1 = t_a * a1 + t_b[0];

  // skip projection if affine ...
  if ((t_b[1][0] != 0.0f) || (t_b[1][1] != 0.0f)) // FIXME -- can be faster with hack
    {
      const float xy0d = dot(t_b[1],a0) + 1.0f; // make sure this resolves to two FMAs

      if (xy0d != 0.0f) // FIXME -- harden this further
        xy0 *= (1.0f/xy0d);

      const float xy1d = dot(t_b[1],a1) + 1.0f; // make sure this resolves to two FMAs

      if (xy1d != 0.0f) // FIXME -- harden this further
        xy1 *= (1.0f/xy1d);
    }

  //
  // clip the line segments here
  //
  // SPN_SUBGROUP_UNIFORM const vec4 cv = clips[SPN_CMD_RASTERIZE_GET_CLIP(cmd)];
  //

  // save the cohort id
  SPN_SUBGROUP_UNIFORM const uint cohort = SPN_CMD_RASTERIZE_GET_COHORT(cmd);

  //
  // start rasterizing the lines
  //

  //
  // how many non-horizontal subpixel y-axis slivers are there?
  //
  const float y_max    = max(xy0.y,xy1.y);
  const float yp_ceil  = ceil(y_max * SPN_TTS_SUBPIXEL_Y_SCALE_DOWN);  // hi pixel

  const float y_min    = min(xy0.y,xy1.y);
  const float yp_floor = floor(y_min * SPN_TTS_SUBPIXEL_Y_SCALE_DOWN); // lo pixel

  const float yp_segs  = yp_ceil - yp_floor; // how many y pixels?

  const bool  y0_lt    = xy0.y <= xy1.y;

  //
  // inclusive subgroup scan of y_segs
  //
  // segs | 33 15 18 13 --
  // -----+---------------
  // inc  | 33 48 66 79 --
  // exc  |  0 33 48 66 79
  //
  float yp_inc   = subgroupInclusiveAdd(yp_segs);
  float yp_count = subgroupBroadcast(yp_inc,SPN_KERNEL_RASTERIZE_SUBGROUP_SIZE-1);

  // return if there is nothing to do
  if (yp_count == 0)
    return;

  int yp_exc = int(yp_inc - yp_segs);

#if 0

  //
  // for (yp_ii=0; yp_ii<y_count; yp_ii+=SPN_KERNEL_RASTERIZE_SUBGROUP_SIZE)
  //  {
  //     yp_delta = yp_ii - yp_exc + gl_SubgroupInvocationID;
  //
  //     yp_exc  -= SPN_KERNEL_RASTERIZE_SUBGROUP_SIZE;
  //  }
  //

  float y_floor = yp_floor * SPN_TTS_SUBPIXEL_Y_SCALE_UP;
  uint  yp_diff = yp_exc - yp_ii;

  while (true)
    {
      smem.scratch[gl_SubgroupInvocationID] = 0;


      if ((yp_diff < SPN_KERNEL_RASTERIZE_SUBGROUP_SIZE) && (yp_inc > yp_ii))
        smem.scratch[max(0,yp_diff)] = gl_SubgroupInvocationID;

      subgroupMemoryBarrierShared(); // read-after-write

      //
      // shuffle in line coords
      //
      uint  lane_y       = subgroupInclusiveMax(smem.scratch[gl_SubgroupInvocationID]);
      uint  lane_delta   = gl_SubgroupInvocationID - subgroupShuffle(yp_diff,lane_y);
      float lane_y_min   = subgroupShuffle(y_min,  lane_y); // lo coord
      float lane_y_floor = subgroupShuffle(y_floor,lane_y); // lo floor
      float lane_y_max   = subgroupShuffle(y_max,lane_y); // hi coord

      while (true)
        {
          float      lane_y_lo  = max(lane_y_min,lane_y_floor+lane_delta);
          float      lane_y_hi  = lane_y_lo; // default is invalid
          const bool is_valid_y = y_base + gl_SubgroupInvocationID < y_count;

          if (is_valid_y)
            lane_y_hi = min(lane_y_hi + 1.0f,lane_y_max);

          // given y/x

          uint yx_hash   = spn_hash(yx);
          uint yx_entry  = smem.entries[yx_hash];

          bool yx_hit = (yx == SPN_RASTERIZE_ENTRY_GET_YX(yx_entry));

          uint yx_match  = spn_match(yx_hash,yx_hit);

          uint yx_idx = SPN_RASTERIZE_ENTRY_GET_COUNT(yx_entry) + spn_match_lt(yx_match);

          // count how many full subblocks there are in this round
          uint yx_tile_idx = yx_idx & SPN_TILE_WIDTH_MASK;
          uint yx_full     = subgroupBallot(yx_tile_idx == 0);

          // these lanes need new subblocks
#if 0
          if (yx_idx >= SPN_TILE_WIDTH)
            {
              smem.subblocks[yx_full_lt
            }
#endif

          // fire and forget the ttrk keys...
          bp_blocks[yx_subblock_id * SPN_BLOCK_POOL_SUBBLOCK_DWORDS + yx_tile_idx] = ttrk;

          // save hash entry {yx,count} and subblock id
          bool is_last_idx = true;
          if (is_last_idx)
            {
              const uint yx_count_new = yx_tile_idx + 1;

              smem.entry.yxc[yx_hash] = SPN_RASTERIZE_ENTRY_MAKE(yx,yx_count_new);

              if (yx_count_new > 0)
                smem.entry.sub[yx_hash] = yx_subblock_id;
            }

          y_base += SPN_KERNEL_RASTERIZE_SUBGROUP_SIZE;

          if (y_base >= y_count)
            break;

          // if all the lanes are still working on the same segment then
          // we can skip a bunch of work
          segs_rem -= SPN_RASTERIZE_SUBGROUP_SIZE;

          if (subgroupAny(segs_rem <= 0))
            break;

          lane_y_lo += SPN_RASTERIZE_SUBGROUP_SIZE;
        }


    }

   // inspect all entries for non-zero counts
   spn_entries_flush();

   //
   spn_block_pool_reclaim(block_pool,block_pool_rem);

#endif

}

//
//
//
//
//
//
//
//
//
//

#if 0

// encode { exc : src } into a uint
#define SPN_RASTERIZE_MAKE_EXC_SRC(exc)                                     \
  ((exc) | (gl_SubgroupInvocationID << (32-SPN_KERNEL_RASTERIZE_SUBGROUP_SIZE_LOG2)))

// get exc_src.exc
#define SPN_RASTERIZE_EXC_SRC_GET_EXC(exc_src)                              \
  bitfieldExtract(exc_src,0,(32-SPN_KERNEL_RASTERIZE_SUBGROUP_SIZE_LOG2))

// get exc_src.src
#define SPN_RASTERIZE_EXC_SRC_GET_SRC(exc_src)                              \
  bitfieldExtract(exc_src,(32-SPN_KERNEL_RASTERIZE_SUBGROUP_SIZE_LOG2),SPN_KERNEL_RASTERIZE_SUBGROUP_SIZE_LOG2)

  const uint exc_src = SPN_RASTERIZE_MAKE_EXC_SRC(y_exc);

  //
  // allocate and init in-register TTSK keys
  //
  spn_uint     sk_v_next = 0;
  spn_ttsk_v_t sk_v;

  sk_v.hi = cohort;

  //
  // initialize smem
  //
  spn_smem_init(smem);

  //
  // initialize blocks / subblocks
  //
  spn_block_id_v_t blocks;
  spn_uint         blocks_next = SPN_RASTERIZE_BLOCK_ID_V_SIZE;

#if SPN_DEVICE_BLOCK_WORDS_LOG2 > SPN_DEVICE_SUBBLOCK_WORDS_LOG2
  spn_block_id_t   subblocks   = 0;
#endif

  //
  // sliver lines
  //
  spn_sliver(bp_atomics,
             bp_blocks,
             bp_ids,
             bp_mask,
             cohort_atomics,
             &subblocks,
             &blocks,
             &blocks_next,
             &sk_v,
             &sk_v_next,
             sk_extent,
             smem,
             l0x,l0y,l1x,l1y);

  //
  // - flush work-in-progress blocks
  // - return unused block ids
  //
  spn_finalize(bp_atomics,
               bp_blocks,
               bp_ids,
               bp_mask,
               cohort_atomics,
               &blocks,
               blocks_next,
               &sk_v,
               sk_v_next,
               sk_extent,
               smem);
#endif







#if 0

//
//
//

#include "tile.h"
#include "common.h"
#include "atomic_cl.h"
#include "block_pool_cl.h"
#include "raster_builder_cl_12.h"
#include "kernel_cl_12.h"

// #define SPN_ARCH_AVX2
// #define SPN_RASTERIZE_SIMD_USES_SMEM

#define PRINTF_ENABLE       0
#define PRINTF_BLOCK_COUNT  0

//
// NOTE:
//
// ON SIMD DEVICES THE BIN COUNT MUST BE POW2 SO THAT WE CAN LOAD IT
// AS A VECTOR AND PERFORM A SWIZZLE/SHUFFLE
//
// NOTE:
//
// IGNORE FOR NOW ANY AVX2 CODE SNIPPETS.  THEY WILL BE MOVED ASAP.
//
//

#if 0 // SPN_ARCH_AVX2

// #define SPN_RASTERIZE_SUBGROUP_SIZE              1
// #define SPN_RASTERIZE_VECTOR_SIZE_LOG2           3
// #define SPN_RASTERIZE_WORKGROUP_COUNT_SUBGROUP   1

// #define SPN_TTXB_WORDS                           8

// #define SPN_RASTERIZE_FLOAT                      float8
// #define SPN_RASTERIZE_UINT                       uint8
// #define SPN_RASTERIZE_INT                        int8
// #define SPN_RASTERIZE_PREDICATE                  int8

// #define SPN_RASTERIZE_BIN_BLOCK                  uint16
// #define SPN_RASTERIZE_BIN                        uint8

// #define SPN_RASTERIZE_POOL                       uint8
// #define SPN_RASTERIZE_POOL_SCALE                 6

// #define SPN_RASTERIZE_TILE_HASH_X_BITS           1
// #define SPN_RASTERIZE_TILE_HASH_Y_BITS           2

// #define SPN_RASTERIZE_VECTOR_EXPAND()            SPN_EXPAND_8()

#endif

//
// SIMT
//

#define SPN_RASTERIZE_BLOCK_ID_V_SIZE        SPN_RASTERIZE_SUBGROUP_SIZE
#define SPN_RASTERIZE_TTSK_V_SIZE            SPN_RASTERIZE_SUBGROUP_SIZE
#define SPN_RASTERIZE_TTSK_V_MASK            (SPN_RASTERIZE_TTSK_V_SIZE - 1)

//
//
//

#define SPN_RASTERIZE_VECTOR_SIZE            (1 << SPN_RASTERIZE_VECTOR_SIZE_LOG2)
#define SPN_RASTERIZE_ELEMS_PER_SUBGROUP     (SPN_RASTERIZE_SUBGROUP_SIZE * SPN_RASTERIZE_VECTOR_SIZE)

//
//
//

#define SPN_RASTERIZE_YX_INIT                0x7FFF7FFF  // { +32767, +32767 }
#define SPN_RASTERIZE_YX_INVALID             0x80008000  // { -32768, -32768 }

//
//
//

#define SPN_RASTERIZE_TILE_HASH_X_MASK       SPN_BITS_TO_MASK(SPN_RASTERIZE_TILE_HASH_X_BITS)
#define SPN_RASTERIZE_TILE_HASH_Y_MASK       SPN_BITS_TO_MASK(SPN_RASTERIZE_TILE_HASH_Y_BITS)
#define SPN_RASTERIZE_TILE_HASH_BITS         (SPN_RASTERIZE_TILE_HASH_X_BITS + SPN_RASTERIZE_TILE_HASH_Y_BITS)
#define SPN_RASTERIZE_TILE_HASH_BIN_COUNT    (1 << SPN_RASTERIZE_TILE_HASH_BITS)
#define SPN_RASTERIZE_TILE_HASH_BIN_BITS     (SPN_RASTERIZE_TILE_HASH_BITS + 1) // FIXME -- LOG2_RU(BIN_COUNT)
#define SPN_RASTERIZE_TILE_HASH_BIN_MASK     SPN_BITS_TO_MASK(SPN_RASTERIZE_TILE_HASH_BIN_BITS)

//
// Norbert Juffa notes: "GPU Pro Tip: Lerp Faster in C++"
//
// https://devblogs.nvidia.com/parallelforall/lerp-faster-cuda/
//
// Lerp in two fma/mad ops:
//
//    t * b + ((-t) * a + a)
//
// Note: OpenCL documents mix() as being implemented as:
//
//    a + (b - a) * t
//
// But this may be a native instruction on some devices. For example,
// on GEN9 there is an LRP "linear interoplation" opcode but it
// doesn't appear to support half floats.
//
// Feel free to toggle this option and then benchmark and inspect the
// generated code.  We really want the double FMA to be generated when
// there isn't support for a LERP/MIX operation.
//

#if 1
#define SPN_LERP(a,b,t)      mad(t,b,mad(-(t),a,a))
#else
#define SPN_LERP(a,b,t)      mix(a,b,t)
#endif

//
// There is no integer MAD in OpenCL with "don't care" overflow
// semantics.
//
// FIXME -- verify if the platform needs explicit MAD operations even
// if a "--fastmath" option is available at compile time.  It might
// make sense to explicitly use MAD calls if the platform requires it.
//

#if 1
#define SPN_MAD_UINT(a,b,c)  ((a) * (b) + (c))
#else
#define SPN_MAD_UINT(a,b,c)  mad_sat(a,b,c)
#endif

//
//
//



//
//
//

union spn_bp_elem
{
  spn_uint              u32;
  spn_tagged_block_id_t tag_id;
  spn_float             coord;
};

//
//
//

struct spn_subgroup_smem
{
  //
  // SIMT subgroup scratchpad for max scan -- also shared with 'winner' member
  //
#if ( SPN_RASTERIZE_SUBGROUP_SIZE > 1 ) || defined ( SPN_RASTERIZE_SIMD_USES_SMEM )
  struct {
    union {

      spn_uint                winner;

      struct {
        spn_uint              scratch[SPN_RASTERIZE_SUBGROUP_SIZE];
      } aN;

      struct {
        SPN_RASTERIZE_UINT    scratch[SPN_RASTERIZE_SUBGROUP_SIZE];
      } vN;
    };
  } subgroup;
#endif

  //
  // work-in-progress TTSB blocks and associated YX keys
  //
  union {
    struct {
      // FIXME -- some typedefs are valid here
      spn_uint                ttsb [SPN_RASTERIZE_TILE_HASH_BIN_COUNT][SPN_DEVICE_SUBBLOCK_WORDS];
      spn_uint                yx   [SPN_RASTERIZE_TILE_HASH_BIN_COUNT];
      spn_uint                id   [SPN_RASTERIZE_TILE_HASH_BIN_COUNT];
      spn_uint                count[SPN_RASTERIZE_TILE_HASH_BIN_COUNT];
    } aN;
#if ( SPN_RASTERIZE_SUBGROUP_SIZE == 1 )
    struct {
      SPN_RASTERIZE_BIN_BLOCK ttsb[SPN_RASTERIZE_TILE_HASH_BIN_COUNT];
      SPN_RASTERIZE_BIN       yx;
      SPN_RASTERIZE_BIN       id;
      SPN_RASTERIZE_BIN       count;
    } vN;
#endif
  } bin;
};

//
//
//

#if ( SPN_RASTERIZE_SUBGROUP_SIZE == 1 )
#define spn_subgroup_lane()  0
#else
#define spn_subgroup_lane()  get_sub_group_local_id()
#endif

//
// replenish block ids
//
// note that you can't overrun the block id pool since it's a ring
//

static
void
spn_blocks_replenish(spn_uint                           * const blocks_next,
                     spn_block_id_v_t                   * const blocks,
                     __global SPN_ATOMIC_UINT  volatile * const bp_atomics,
                     spn_uint                             const bp_mask, // pow2 modulo mask for block pool ring
                     __global spn_block_id_t   const    * const bp_ids)
{
  //
  // get a new vector of block ids -- this is kind of a narrow
  // allocation but subblocks help stretch out the pool.
  //
  // FIXME -- there is now plenty of SMEM to allocate a LOT of block ids
  //
  spn_uint bp_idx = 0;

  if (spn_subgroup_lane() == 0)
    {
      bp_idx = SPN_ATOMIC_ADD_GLOBAL_RELAXED_DEVICE(bp_atomics+SPN_BP_ATOMIC_OFFSET_READS,
                                                    SPN_RASTERIZE_BLOCK_ID_V_SIZE); // ring_reads
#if 0
      printf("r+: %8u + %u\n",bp_idx,SPN_RASTERIZE_BLOCK_ID_V_SIZE);
#endif
    }

  bp_idx       = (sub_group_broadcast(bp_idx,0) + spn_subgroup_lane()) & bp_mask;
  *blocks      = bp_ids[bp_idx];
  *blocks_next = 0;
}

//
//
//

static
spn_block_id_t
spn_blocks_get_next(spn_uint                           * const blocks_next,
                    spn_block_id_v_t                   * const blocks,
                    __global SPN_ATOMIC_UINT  volatile * const bp_atomics,
                    spn_uint                             const bp_mask, // pow2 modulo mask for block pool ring
                    __global spn_block_id_t   const    * const bp_ids)
{
  // replenish?
  if (*blocks_next == SPN_RASTERIZE_BLOCK_ID_V_SIZE)
    {
      spn_blocks_replenish(blocks_next,blocks,bp_atomics,bp_mask,bp_ids);
    }

#if ( SPN_RASTERIZE_SUBGROUP_SIZE > 1 )
  //
  // SIMT
  //
  spn_block_id_t id = sub_group_broadcast(*blocks,*blocks_next);

#else
  //
  // SIMD
  //
  spn_block_id_t id = blocks->s0;

  spn_shuffle_down_1(*blocks);

#endif

  *blocks_next += 1;

  return id;
}

//
// subblock allocator
//

#if SPN_DEVICE_BLOCK_WORDS_LOG2 > SPN_DEVICE_SUBBLOCK_WORDS_LOG2

static
spn_block_id_t
spn_subblocks_get_next(spn_block_id_t                     * const subblocks,
                       spn_uint                           * const blocks_next,
                       spn_block_id_v_t                   * const blocks,
                       __global SPN_ATOMIC_UINT  volatile * const bp_atomics,
                       spn_uint                             const bp_mask, // pow2 modulo mask for block pool ring
                       __global spn_block_id_t   const    * const bp_ids)
{
  if ((*subblocks & SPN_DEVICE_SUBBLOCKS_PER_BLOCK_MASK) == 0)
    {
      *subblocks = spn_blocks_get_next(blocks_next,blocks,bp_atomics,bp_mask,bp_ids);
    }

  spn_block_id_t const sb_id = *subblocks;

  *subblocks += 1;

#if 0
  if (get_sub_group_local_id() == 0)
    printf("= %u\n",sb_id);
#endif

  return sb_id;
}


#define SPN_SUBBLOCKS_BLOCKS_PROTO() spn_block_id_t * const subblocks, spn_block_id_t * const blocks
#define SPN_SUBBLOCKS_BLOCKS_ARGS()  subblocks, blocks

#else

#define SPN_SUBBLOCKS_BLOCKS_PROTO() spn_block_id_t * const blocks
#define SPN_SUBBLOCKS_BLOCKS_ARGS()  blocks

#endif

//
//
//

static
spn_block_id_t
spn_ttsk_v_append(SPN_SUBBLOCKS_BLOCKS_PROTO(),
                  spn_uint                           * const blocks_next,
                  __global SPN_ATOMIC_UINT  volatile * const bp_atomics,
                  spn_uint                             const bp_mask, // pow2 modulo mask for block pool ring
                  __global spn_block_id_t   const    * const bp_ids,
                  __global SPN_ATOMIC_UINT  volatile * const cohort_atomics,
                  spn_ttsk_v_t                       * const sk_v,
                  spn_uint                           * const sk_v_next,
                  __global spn_ttsk_s_t              * const sk_extent,
                  spn_uint                             const new_yx)
{
#if SPN_DEVICE_BLOCK_WORDS_LOG2 > SPN_DEVICE_SUBBLOCK_WORDS_LOG2
  spn_block_id_t const new_id = spn_subblocks_get_next(subblocks,
                                                       blocks_next,
                                                       blocks,
                                                       bp_atomics,
                                                       bp_mask,
                                                       bp_ids);
#else
  spn_block_id_t const new_id = spn_blocks_get_next(blocks_next,
                                                    blocks,
                                                    bp_atomics,
                                                    bp_mask, // pow2 modulo mask for block pool ring
                                                    bp_ids);
#endif

  if (get_sub_group_local_id() == (*sk_v_next & SPN_RASTERIZE_TTSK_V_MASK))
    {
      sk_v->lo = new_id;
      sk_v->hi = (sk_v->hi & SPN_TTRK_HI_MASK_COHORT) | new_yx;
#if 0
      printf("@ ( %3u, %3u ) %u\n",
             (new_yx >> 12) & 0xFFF,
             (new_yx      ) & 0xFFF,
             new_id);
#endif
    }

  *sk_v_next += 1;

  if (*sk_v_next == SPN_RASTERIZE_TTSK_V_SIZE)
    {
      *sk_v_next = 0;

      spn_uint sk_idx = 0;

      if (spn_subgroup_lane() == 0)
        {
          sk_idx = SPN_ATOMIC_ADD_GLOBAL_RELAXED_DEVICE
            (cohort_atomics+SPN_RASTER_COHORT_ATOMIC_OFFSET_KEYS,SPN_RASTERIZE_TTSK_V_SIZE);
#if 0
          printf("+ %u\n",sk_idx);
#endif
        }

      sk_idx = sub_group_broadcast(sk_idx,0) + spn_subgroup_lane();

#if ( SPN_RASTERIZE_SUBGROUP_SIZE > SPN_RASTERIZE_TTSK_V_SIZE )
      if (spn_subgroup_lane() < SPN_RASTERIZE_TTSK_V_SIZE)
#endif
        {
          sk_extent[sk_idx] = *sk_v;
#if 0
          printf("> %u : %v2u\n",sk_idx,*sk_v);
#endif
        }
    }

  return new_id;
}

//
//
//

static
SPN_RASTERIZE_FLOAT
spn_subgroup_scan_inclusive_add_float(const float v)
{
#if ( SPN_RASTERIZE_SUBGROUP_SIZE == 1 )
  //
  // SIMD
  //
  // Note that there isn't a built-in horizontal scan for vectors so
  // we'll define some here for various widths.
  //
  // FIXME -- a scalar version might be faster so put in a
  // compile-time switch to selection between implementations
  //

#if   ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 0 )
  return v;

#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 1 )
  // 01
  //  0 +
  // --
  // 01
  const float w = mad(v.s10,(SPN_RASTERIZE_FLOAT)(0,1),v);
  return w;

#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 2 )
  // 0123
  //  012 +
  // ----
  // 0123
  //   01 +
  // ----
  // 0123
  //
  const float w = mad(v.s3012,(SPN_RASTERIZE_FLOAT)(0,1,1,1),v);
  const float x = mad(w.s2301,(SPN_RASTERIZE_FLOAT)(0,0,1,1),w);
  return x;

#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 3 )
  // 01234567
  //  0123456 +
  // --------
  // 01234567
  //   012345 +
  // --------
  // 01234567
  //     0123 +
  // --------
  // 01234567
  //
  const float w = mad(v.s70123456,(SPN_RASTERIZE_FLOAT)(0,1,1,1,1,1,1,1),v);
  const float x = mad(w.s67012345,(SPN_RASTERIZE_FLOAT)(0,0,1,1,1,1,1,1),w);
  const float y = mad(x.s45670123,(SPN_RASTERIZE_FLOAT)(0,0,0,0,1,1,1,1),x);
  return y;

#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 4 )
  // 0123456789abcdef
  //  0123456789abcde +
  // ----------------
  // 0123456789abcdef
  //   0123456789abcd +
  // ----------------
  // 0123456789abcdef
  //     0123456789ab +
  // ----------------
  // 0123456789abcdef
  //         01234567 +
  // ----------------
  // 0123456789abcdef
  //
  const float w = mad(v.sf0123456789abcde,(SPN_RASTERIZE_FLOAT)(0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1),v);
  const float x = mad(w.sef0123456789abcd,(SPN_RASTERIZE_FLOAT)(0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1),w);
  const float y = mad(x.scdef0123456789ab,(SPN_RASTERIZE_FLOAT)(0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1),x);
  const float z = mad(y.s89abcdef01234567,(SPN_RASTERIZE_FLOAT)(0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1),y);
  return z;

#endif

#else
  //
  // SIMT
  //

  return sub_group_scan_inclusive_add(v);

#endif
}

//
//
//

static
SPN_RASTERIZE_UINT
spn_subgroup_scan_inclusive_add_uint(SPN_RASTERIZE_UINT const v)
{
#if ( SPN_RASTERIZE_SUBGROUP_SIZE == 1 )
  //
  // SIMD
  //
  // Note that there isn't a built-in horizontal scan for vectors so
  // we'll define some here for various widths.
  //
  // FIXME -- a scalar version might be faster so put in a
  // compile-time switch to selection between implementations
  //

#if   ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 0 )
  return v;

#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 1 )
  // 01
  //  0 +
  // --
  // 01
  SPN_RASTERIZE_UINT const w = SPN_MAD_UINT(v.s10,(SPN_RASTERIZE_UINT)(0,1),v);
  return w;

#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 2 )
  // 0123
  //  012 +
  // ----
  // 0123
  //   01 +
  // ----
  // 0123
  //
  SPN_RASTERIZE_UINT const w = SPN_MAD_UINT(v.s3012,(SPN_RASTERIZE_UINT)(0,1,1,1),v);
  SPN_RASTERIZE_UINT const x = SPN_MAD_UINT(w.s2301,(SPN_RASTERIZE_UINT)(0,0,1,1),w);
  return x;

#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 3 )
  // 01234567
  //  0123456 +
  // --------
  // 01234567
  //   012345 +
  // --------
  // 01234567
  //     0123 +
  // --------
  // 01234567
  //
  SPN_RASTERIZE_UINT const w = SPN_MAD_UINT(v.s70123456,(SPN_RASTERIZE_UINT)(0,1,1,1,1,1,1,1),v);
  SPN_RASTERIZE_UINT const x = SPN_MAD_UINT(w.s67012345,(SPN_RASTERIZE_UINT)(0,0,1,1,1,1,1,1),w);
  SPN_RASTERIZE_UINT const y = SPN_MAD_UINT(x.s45670123,(SPN_RASTERIZE_UINT)(0,0,0,0,1,1,1,1),x);
  return y;

#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 4 )
  // 0123456789abcdef
  //  0123456789abcde +
  // ----------------
  // 0123456789abcdef
  //   0123456789abcd +
  // ----------------
  // 0123456789abcdef
  //     0123456789ab +
  // ----------------
  // 0123456789abcdef
  //         01234567 +
  // ----------------
  // 0123456789abcdef
  //
  SPN_RASTERIZE_UINT const w = SPN_MAD_UINT(v.sf0123456789abcde,(SPN_RASTERIZE_UINT)(0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1),v);
  SPN_RASTERIZE_UINT const x = SPN_MAD_UINT(w.sef0123456789abcd,(SPN_RASTERIZE_UINT)(0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1),w);
  SPN_RASTERIZE_UINT const y = SPN_MAD_UINT(x.scdef0123456789ab,(SPN_RASTERIZE_UINT)(0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1),x);
  SPN_RASTERIZE_UINT const z = SPN_MAD_UINT(y.s89abcdef01234567,(SPN_RASTERIZE_UINT)(0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1),y);
  return z;

#endif

#else
  //
  // SIMT
  //

  return sub_group_scan_inclusive_add(v);

#endif
}

//
//
//

static
SPN_RASTERIZE_UINT
spn_subgroup_scan_inclusive_max(SPN_RASTERIZE_UINT const v)
{
#if ( SPN_RASTERIZE_SUBGROUP_SIZE == 1 )
  //
  // SIMD
  //
  // Note that there isn't a built-in horizontal scan for vectors so
  // we'll define some here for various widths.
  //
  // FIXME -- a scalar version might be faster so put in a
  // compile-time switch to selection between implementations
  //

#if   ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 0 )
  return v;

#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 1 )
  // 01
  // 00 max
  // --
  // 01
  SPN_RASTERIZE_UINT const w = max(v.s00,v);
  return w;

#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 2 )
  // 0123
  // 0012 +
  // ----
  // 0123
  // 0101 +
  // ----
  // 0123
  //
  SPN_RASTERIZE_UINT const w = max(v.s0012,v);
  SPN_RASTERIZE_UINT const x = max(w.s0101,w);
  return x;

#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 3 )
  // 01234567
  // 00123456 +
  // --------
  // 01234567
  // 01012345 +
  // --------
  // 01234567
  // 01230123 +
  // --------
  // 01234567
  //
  SPN_RASTERIZE_UINT const w = max(v.s00123456,v);
  SPN_RASTERIZE_UINT const x = max(w.s01012345,w);
  SPN_RASTERIZE_UINT const y = max(x.s01230123,x);
  return y;

#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 4 )
  // 0123456789abcdef
  // 00123456789abcde +
  // ----------------
  // 0123456789abcdef
  // 010123456789abcd +
  // ----------------
  // 0123456789abcdef
  // 01230123456789ab +
  // ----------------
  // 0123456789abcdef
  // 0123456701234567 +
  // ----------------
  // 0123456789abcdef
  //
  SPN_RASTERIZE_UINT const w = max(v.s00123456789abcde,v);
  SPN_RASTERIZE_UINT const x = max(w.s010123456789abcd,w);
  SPN_RASTERIZE_UINT const y = max(x.s01230123456789ab,x);
  SPN_RASTERIZE_UINT const z = max(y.s0123456701234567,y);
  return z;

#endif

#else
  //
  // SIMT
  //

  return sub_group_scan_inclusive_max(v);

#endif
}

//
//
//

static
float
spn_subgroup_last_float(const float v)
{
#if ( SPN_RASTERIZE_SUBGROUP_SIZE == 1 )
  //
  // SIMD
  //
#if   ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 0 )
  return v;
#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 1 )
  return v.s1;
#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 2 )
  return v.s3;
#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 3 )
  return v.s7;
#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 4 )
  return v.sf;
#endif

#else
  //
  // SIMT
  //
  return sub_group_broadcast(v,SPN_RASTERIZE_SUBGROUP_SIZE-1);

#endif
}

//
//
//

static
SPN_RASTERIZE_UINT
spn_subgroup_last_uint(SPN_RASTERIZE_UINT const v)
{
#if ( SPN_RASTERIZE_SUBGROUP_SIZE == 1 )
  //
  // SIMD
  //
#if   ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 0 )
  return v;
#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 1 )
  return v.s1;
#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 2 )
  return v.s3;
#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 3 )
  return v.s7;
#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 4 )
  return v.sf;
#endif

#else
  //
  // SIMT
  //
  return sub_group_broadcast(v,SPN_RASTERIZE_SUBGROUP_SIZE-1);

#endif
}

//
//
//

static
float
spn_subgroup_first(const float v)
{
#if ( SPN_RASTERIZE_SUBGROUP_SIZE == 1 )
  //
  // SIMD
  //
#if   ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 0 )
  return v;
#else
  return v.s0;
#endif

#else
  //
  // SIMT
  //
  return sub_group_broadcast(v,0);

#endif
}

//
//
//

static
SPN_RASTERIZE_FLOAT
spn_subgroup_shuffle(const float v,
                      SPN_RASTERIZE_UINT  const i)
{
#if ( SPN_RASTERIZE_SUBGROUP_SIZE == 1 )
  //
  // SIMD
  //
#if   ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 0 )
  return v;
#else
  return shuffle(v,i);
#endif

#else
  //
  // SIMT
  //
  return intel_sub_group_shuffle(v,i);

#endif
}

//
//
//

static
SPN_RASTERIZE_FLOAT
spn_subgroup_shuffle_up_1(const float p, // previous
                          const float c) // current
{
#if ( SPN_RASTERIZE_SUBGROUP_SIZE == 1 )
  //
  // SIMD
  //
  // FIXME -- there are alternative formulations here:
  //
  // Option 1:
  //
  //   select(c.rotate(+1),p.rotate(-1),(1,0,0,...))
  //
  // Option 2:
  //
  //   p is a scalar
  //   t    = c.rotate(+1)
  //   t.s0 = p;
  //
  // Option 3: ...
  //
#if   ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 0 )
  return p;
#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 1 )
  return shuffle2(p,c,(uint2)(1,2));
#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 2 )
  return shuffle2(p,c,(uint4)(3,4,5,6));
#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 3 )
  return shuffle2(p,c,(uint8)(7,8,9,10,11,12,13,14));
#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 4 )
  return shuffle2(p,c,(uint16)(15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30));
#endif

#else
  //
  // SIMT
  //
  return intel_sub_group_shuffle_up(p,c,1);

#endif
}

//
//
//

static
bool
spn_is_lane_first()
{
#if ( SPN_RASTERIZE_SUBGROUP_SIZE == 1)
  //
  // SIMD
  //
  return true;
#else
  //
  // SIMT
  //
  return get_sub_group_local_id() == 0;
#endif
}

//
//
//

static
SPN_RASTERIZE_FLOAT
spn_delta_offset()
{
#if ( SPN_RASTERIZE_SUBGROUP_SIZE == 1 )
  //
  // SIMD
  //
#if   ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 0 )
  return 1;
#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 1 )
  return (SPN_RASTERIZE_FLOAT)( 1, 2 );
#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 2 )
  return (SPN_RASTERIZE_FLOAT)( 1, 2, 3, 4 );
#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 3 )
  return (SPN_RASTERIZE_FLOAT)( 1, 2, 3, 4, 5, 6, 7, 8 );
#elif ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 4 )
  return (SPN_RASTERIZE_FLOAT)( 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16 );
#endif

#else
  //
  // SIMT
  //
  return 1.0f + get_sub_group_local_id();

#endif

}

//
//
//

static
int
spn_subgroup_any(SPN_RASTERIZE_PREDICATE const p)
{
#if ( SPN_RASTERIZE_SUBGROUP_SIZE == 1 )
  //
  // SIMD
  //
  return any(p);
#else
  //
  // SIMT
  //
  return sub_group_any(p);
#endif
}

//
//
//

#define SPN_PATH_NODEWORD_IS_LAST(n)  (((n) & SPN_DEVICE_BLOCK_WORDS_MASK) == SPN_DEVICE_BLOCK_WORDS_MASK)

void
spn_segment_next(__global union spn_bp_elem * const bp_blocks,
                 spn_uint                   * const nodeword,
                 spn_block_id_t             * const id)
{
  if ((++*id & SPN_DEVICE_SUBBLOCKS_PER_BLOCK_MASK) == 0)
    {
      if (SPN_PATH_NODEWORD_IS_LAST(++*nodeword))
        {
          *nodeword = SPN_TAGGED_BLOCK_ID_GET_ID(bp_blocks[*nodeword].tag_id) * SPN_DEVICE_SUBBLOCK_WORDS;
        }

      spn_tagged_block_id_t const tag_id = bp_blocks[*nodeword].tag_id;

      *id = SPN_TAGGED_BLOCK_ID_GET_ID(tag_id);
    }
}

//
//
//

static
SPN_RASTERIZE_FLOAT
spn_native_length(const float x, const float y)
{
  return native_sqrt(x * x + y * y);
}

//
// Wang's Formula (1985)
//

#define SPN_WANG_PIXEL_RESL   0.25f // <-- this can be tuned

#define SPN_WANG_EPSILON      (SPN_WANG_PIXEL_RESL * SPN_SUBPIXEL_RESL_X_F32)

#define SPN_WANG_CUBIC        ((3.0f * 2.0f) / (8.0f * SPN_WANG_EPSILON))
#define SPN_WANG_QUADRATIC    ((2.0f       ) / (8.0f * SPN_WANG_EPSILON))

#define SPN_WANG_LENGTH(x,y)  spn_native_length(x,y)
#define SPN_WANG_SQRT(x)      native_sqrt(x)

//
//
//

static
SPN_RASTERIZE_FLOAT
spn_wangs_formula_cubic(const float t0x, const float t0y,
                        const float t1x, const float t1y,
                        const float t2x, const float t2y,
                        const float t3x, const float t3y)
{
  //
  // Return the number of evenly spaced (in the parametric sense) line
  // segments that are guaranteed to be within "epsilon" error of the
  // curve.
  //
  // We're then going to take multiples of the reciprocal of this
  // number so that the segmentation can be distributed across the
  // subgroup.
  //
  // Note, this can probably be slightly optimized per architecture
  // but it's probably far from being a hotspot since it's all
  // straight-line unpredicated code.
  //
  // The result is an integer ranging from [1.0,#segments]
  //
  // Note that even if all of the control points are coincident, the
  // max(1.0f) will categorize this as a line of 1 segment.
  //
  // This is what we want!  We want to convert cubics to lines as
  // easily as possible and *then* cull lines that are either
  // horizontal or zero length.
  //
  return max(1.0f,
             ceil(SPN_WANG_SQRT(SPN_WANG_CUBIC *
                                SPN_WANG_LENGTH(max(fabs(t2x - 2.0f * t1x + t0x),
                                                    fabs(t3x - 2.0f * t2x + t1x)),
                                                max(fabs(t2y - 2.0f * t1y + t0y),
                                                    fabs(t3y - 2.0f * t2y + t1y))))));
}

static
SPN_RASTERIZE_FLOAT
spn_wangs_formula_quadratic(const float t0x, const float t0y,
                            const float t1x, const float t1y,
                            const float t2x, const float t2y)
{
  return max(1.0f,
             ceil(SPN_WANG_SQRT(SPN_WANG_QUADRATIC *
                                SPN_WANG_LENGTH(t2x - 2.0f * t1x + t0x,
                                                t2y - 2.0f * t1y + t0y))));
}

//
// rational curves
//

static
SPN_RASTERIZE_FLOAT
spn_wangs_formula_cubic_rat()
{
  return 0.0f;
}

static
SPN_RASTERIZE_FLOAT
spn_wangs_formula_quad_rat()
{
  return 0.0f;
}

//
// flush any work-in-progress blocks and return unused block ids
//

static
void
spn_finalize(__global SPN_ATOMIC_UINT          volatile * const bp_atomics,
             __global union spn_bp_elem                 * const bp_blocks,
             __global uint                              * const bp_ids,
             spn_uint                                     const bp_mask,
             __global SPN_ATOMIC_UINT          volatile * const cohort_atomics,
             spn_block_id_v_t                           * const blocks,
             spn_uint                                     const blocks_next,
             spn_ttsk_v_t                               * const sk_v,
             spn_uint                                     const sk_v_next,
             __global spn_ttsk_s_t                      * const sk_extent,
             __local  struct spn_subgroup_smem volatile * const smem)
{
  //
  // flush non-empty bins
  //
  // FIXME -- accelerate this iteration/search with a subgroup operation
  //
  for (spn_uint ii=0; ii<SPN_RASTERIZE_TILE_HASH_BIN_COUNT; ii++)
    {
      if (smem->bin.aN.count[ii] > 0)
        {
          spn_block_id_v_t const id  = smem->bin.aN.id[ii];
          spn_uint         const idx = id * SPN_DEVICE_SUBBLOCK_WORDS + spn_subgroup_lane();
          spn_uint         const tts = smem->bin.aN.ttsb[ii][spn_subgroup_lane()];
#if 0
          printf("???????? : [ %10u = %10u : %08X ]\n",id,idx,tts);
#endif
          bp_blocks[idx].u32 = tts;
        }

      //
      // FIXME -- vectorize with vstoreN()
      //
    }

  //
  // return remaining block ids back to the pool
  //
  spn_uint const blocks_rem = SPN_RASTERIZE_BLOCK_ID_V_SIZE - blocks_next;

  if (blocks_rem > 0)
    {
      spn_uint bp_idx = 0;

      if (spn_subgroup_lane() == 0)
        {
          bp_idx = SPN_ATOMIC_ADD_GLOBAL_RELAXED_DEVICE(bp_atomics+SPN_BP_ATOMIC_OFFSET_WRITES,blocks_rem);

#if 0
          printf("r-: %8u + %u\n",bp_idx,blocks_rem);
#endif
        }

      bp_idx = (sub_group_broadcast(bp_idx,0) + spn_subgroup_lane() - blocks_next) & bp_mask;

      if (spn_subgroup_lane() >= blocks_next)
        {
          bp_ids[bp_idx] = *blocks;
        }
    }

  //
  // flush work-in-progress ryx keys
  //
  if (sk_v_next > 0)
    {
      spn_uint sk_idx = 0;

      if (spn_subgroup_lane() == 0)
        {
          sk_idx = SPN_ATOMIC_ADD_GLOBAL_RELAXED_DEVICE
            (cohort_atomics+SPN_RASTER_COHORT_ATOMIC_OFFSET_KEYS,sk_v_next);
#if 0
          printf("* %u\n",sk_idx);
#endif
        }

      sk_idx = sub_group_broadcast(sk_idx,0) + spn_subgroup_lane();

      if (spn_subgroup_lane() < sk_v_next)
        {
          sk_extent[sk_idx] = *sk_v;
        }
    }
}

//
// If there are lanes that were unable to append to a bin because
// their hashes collided with a bin's current ryx key then those bins
// must be ejected.
//
// Note that we do not eject "full" bins because lazily waiting for a
// collision results in simpler code.
//

static
void
spn_flush(__global SPN_ATOMIC_UINT          volatile * const bp_atomics,
          __global union spn_bp_elem                 * const bp_blocks,
          __global uint                              * const bp_ids,
          spn_uint                                     const bp_mask,
          __global SPN_ATOMIC_UINT          volatile * const cohort_atomics,
          spn_block_id_t                             * const subblocks,
          spn_block_id_v_t                           * const blocks,
          spn_uint                                   * const blocks_next,
          spn_ttsk_v_t                               * const sk_v,
          spn_uint                                   * const sk_v_next,
          __global spn_ttsk_s_t                      * const sk_extent,
          __local  struct spn_subgroup_smem volatile * const smem,
          SPN_RASTERIZE_UINT                           const hash,
          SPN_RASTERIZE_UINT                           const yx,
          SPN_RASTERIZE_PREDICATE                            is_collision) // pass by value
{
#if ( SPN_RASTERIZE_SUBGROUP_SIZE == 1 )
  //
  // SIMD
  //

  //
  // FIXME -- this code is now stale with the changes to the
  // subblock/block allocation strategy
  //

  //
  // get local TTSB ID queue count
  //
  spn_uint ttsb_id_count  = smem->pool.count; // scalar

  // init hash bit mask
  spn_uint component_mask = 0;

  for (int cc=0; cc<SPN_RASTERIZE_VECTOR_SIZE; cc++)
    {
      // if no collision continue
      if (((int*)&is_collision)[cc] == 0)
        continue;

      uint const winner        = ((uint*)&hash)[cc];
      uint const component_bit = 1u << winner;

      // if already processed this hash then continue
      if (component_mask & component_bit)
        continue;

      // update component mask
      component_mask |= component_bit;

      //
      // new winner requires ejecting the old TTSB
      //
      if (smem->bin.aN.count[winner] > 0)
        {
          spn_uint const elem_idx = smem->bin.aN.id[winner] * SPN_DEVICE_SUBBLOCK_WORDS + spn_subgroup_lane();

          bp_blocks[elem_idx].u32 = smem->bin.aN.ttsb[winner][spn_subgroup_lane()];
        }

        //
        // ensure there is at least one TTSK and TTSB ID
        //
        if (ttsb_id_count == SPN_RASTERIZE_POOL_SIZE)
          {
            //
            // update remaining count
            //
            ttsb_id_count = 0;

            //
            // flush accumulated ttsk_ryx keys
            //
            uint const idx = SPN_ATOMIC_ADD_GLOBAL_RELAXED_DEVICE
              (cohort_atomics+SPN_RASTER_COHORT_ATOMIC_OFFSET_KEYS,SPN_RASTERIZE_POOL_SIZE); // ttsk_ryx_count

#if 0
            printf("# %u\n",idx);
#endif

            for (uint ii=0; ii<SPN_RASTERIZE_POOL_SIZE; ii+=SPN_RASTERIZE_SUBGROUP_SIZE)
              {
                ttsk_ryx[idx + ii] = spn_make_ttsk_ryx(smem,SPN_CMD_RASTERIZE_GET_COHORT(cmd),ii);
              }

            //
            // allocate more ttsb ids from pool
            //
            uint const id = SPN_ATOMIC_ADD_GLOBAL_RELAXED_DEVICE(bp_atomics+0,SPN_RASTERIZE_POOL_SIZE); // ring_reads

            for (uint ii=0; ii<SPN_RASTERIZE_POOL_SIZE; ii+=SPN_RASTERIZE_SUBGROUP_SIZE)
              smem->pool.aN.id[ii] = bp_ids[id + ii];
          }

      //
      // invalidate the winning block
      //

      //
      // update bin with winning yx, new ttsb id and zero count
      //
      // all lanes are loading/storing from/to the same index
      //
      smem->bin.vN.ttsb [winner] = ( SPN_TTS_INVALID );
      smem->bin.aN.id   [winner] = smem->pool.aN.id[ttsb_id_count];
      smem->bin.aN.yx   [winner] = smem->pool.aN.yx[ttsb_id_count] = ((uint*)&yx)[cc];
      smem->bin.aN.count[winner] = 0;

      //
      // update count
      //
      ttsb_id_count += 1;
    }

  //
  // save count
  //
  smem->pool.count = ttsb_id_count;

#else
  //
  // SIMT
  //

  do {
    //
    // only one lane will win!
    //
    if (is_collision)
      smem->subgroup.winner = hash;

    barrier(CLK_LOCAL_MEM_FENCE);

    //
    // which bin is being ejected?
    //
    spn_uint const winner = smem->subgroup.winner;

    //
    // which colliding hash is taking over the bin?
    //
    SPN_RASTERIZE_PREDICATE const is_winner = is_collision && (hash == winner);

    //
    // all lanes with the same hash will try to store but only one
    // lane will win
    //
    if (is_winner)
      smem->subgroup.winner = yx;

    barrier(CLK_LOCAL_MEM_FENCE);

    //
    // flush this block to the pool
    //
    if (smem->bin.aN.count[winner] > 0)
      {
        spn_block_id_v_t const id  = smem->bin.aN.id[winner];
        spn_uint         const idx = id * SPN_DEVICE_SUBBLOCK_WORDS + spn_subgroup_lane();
        spn_uint         const tts = smem->bin.aN.ttsb[winner][spn_subgroup_lane()];
#if 0
        printf("%08X : [ %10u = %10u : %08X ]\n",yx,id,idx,tts);
#endif
        bp_blocks[idx].u32 = tts;
      }

    //
    // append new ttsk
    //
    spn_uint       const new_yx = smem->subgroup.winner;
    spn_block_id_t const new_id = spn_ttsk_v_append(SPN_SUBBLOCKS_BLOCKS_ARGS(),
                                                    blocks_next,
                                                    bp_atomics,
                                                    bp_mask, // pow2 modulo mask for block pool ring
                                                    bp_ids,
                                                    cohort_atomics,
                                                    sk_v,
                                                    sk_v_next,
                                                    sk_extent,
                                                    new_yx);

#if 0
    if (get_sub_group_local_id() == 0) {
      printf(">>> %9u\n",new_id);
    }
#endif

    //
    // update bin with winning yx, new ttsb id and zero count
    //
    smem->bin.aN.ttsb [winner][spn_subgroup_lane()] = SPN_TTS_INVALID;
    smem->bin.aN.yx   [winner]                      = new_yx;
    smem->bin.aN.id   [winner]                      = new_id;
    smem->bin.aN.count[winner]                      = 0;

    //
    // remove all lanes matching this hash
    //
    is_collision = is_collision && !is_winner;

    //
    // exit if nothing left to do
    //
  } while (sub_group_any(is_collision));

#endif
}

//
// scatter scan max
//
static
SPN_RASTERIZE_UINT
spn_scatter_scan_max(__local struct spn_subgroup_smem volatile * const smem,
                     SPN_RASTERIZE_FLOAT                         const iss,
                     SPN_RASTERIZE_FLOAT                         const ess)
{
  //
  // prefix sums determine which lanes we're going to work on next
  //
  SPN_RASTERIZE_PREDICATE const is_scratch_store = (iss > 0.0f) && (ess < (float)SPN_RASTERIZE_ELEMS_PER_SUBGROUP);
  SPN_RASTERIZE_UINT      const scratch_idx      = SPN_CONVERT(SPN_RASTERIZE_UINT)(max(ess,0.0f));

#if ( SPN_RASTERIZE_SUBGROUP_SIZE == 1 )
  //
  // SIMD
  //
#ifdef SPN_RASTERIZE_SIMD_USES_SMEM
  //
  // SIMD APPROACH 1: SIMT'ISH
  //

  // zero the volatile smem scratchpad using vector syntax
  smem->subgroup.vN.scratch[0] = ( 0 );

#undef  SPN_EXPAND_X
#define SPN_EXPAND_X(I,S,C,P,A)                         \
  if (is_scratch_store C)                               \
    smem->subgroup.aN.scratch[scratch_idx C] = I;

  SPN_RASTERIZE_VECTOR_EXPAND();

  // propagate lanes to right using max scan
  SPN_RASTERIZE_UINT const scratch = smem->subgroup.vN.scratch[0];
  SPN_RASTERIZE_UINT const source  = spn_subgroup_scan_inclusive_max(scratch);

#else
  //
  // SIMD APPROACH 2: SCALAR'ISH
  //

  SPN_RASTERIZE_UINT source = ( 0 );

#undef  SPN_EXPAND_X
#define SPN_EXPAND_X(I,S,C,P,A)                 \
  if (is_scratch_store C)                       \
    ((uint *)&source)[scratch_idx C] = I;

  SPN_RASTERIZE_VECTOR_EXPAND();

  for (uint ii=1; ii<SPN_RASTERIZE_ELEMS_PER_SUBGROUP; ii++)
    ((uint *)&source)[ii] = max(((uint *)&source)[ii-1],((uint *)&source)[ii]);
#endif

#else
  //
  // SIMT
  //

  //
  // zero the volatile smem scratchpad using vector syntax
  //
  smem->subgroup.vN.scratch[spn_subgroup_lane()] = ( 0 );

  //
  // store source lane at starting lane
  //
  if (is_scratch_store)
    smem->subgroup.aN.scratch[scratch_idx] = spn_subgroup_lane();

  //
  // propagate lanes to right using max scan
  //
  SPN_RASTERIZE_UINT const scratch = smem->subgroup.vN.scratch[spn_subgroup_lane()];
  SPN_RASTERIZE_UINT const source  = spn_subgroup_scan_inclusive_max(scratch);
#endif

  return source;
}

//
// sliver lines into subpixels
//

static
void
spn_sliver(__global SPN_ATOMIC_UINT          volatile * const bp_atomics,
           __global union spn_bp_elem                 * const bp_blocks,
           __global uint                              * const bp_ids,
           spn_uint                                     const bp_mask,
           __global SPN_ATOMIC_UINT          volatile * const cohort_atomics,
           spn_block_id_t                             * const subblocks,
           spn_block_id_v_t                           * const blocks,
           spn_uint                                   * const blocks_next,
           spn_ttsk_v_t                               * const sk_v,
           spn_uint                                   * const sk_v_next,
           __global spn_ttsk_s_t                      * const sk_extent,
           __local  struct spn_subgroup_smem volatile * const smem,
           SPN_RASTERIZE_FLOAT                          const l0x,
           SPN_RASTERIZE_FLOAT                          const l0y,
           SPN_RASTERIZE_FLOAT                          const l1x,
           SPN_RASTERIZE_FLOAT                          const l1y)
{
  //
  // Y-SLIVERING
  // -----------
  //
  // immediately sliver all multi-pixel lines in into 1-pixel high
  // lines
  //
  // note this implicitly squelches horizontal lines
  //
  // there is another test for horizontal lines after x-slivering
  // is complete
  //

  //
  // will we need to flip the sign of y_delta ?
  //
  SPN_RASTERIZE_PREDICATE const y_lt   = (l0y <= l1y);
  SPN_RASTERIZE_UINT      const dy_xor = y_lt ? 0 : 0x80000000;

  //
  // save 1/dy
  //
  const float y_denom = native_recip(l1y - l0y);

  //
  // how many non-horizontal subpixel y-axis slivers are there?
  //
  const float y_min   = floor(fmin(l0y,l1y) * SPN_SUBPIXEL_Y_SCALE_DOWN);
  const float y_max   = ceil (fmax(l0y,l1y) * SPN_SUBPIXEL_Y_SCALE_DOWN);
  const float y_base  = y_lt ? y_min : y_max;
  SPN_RASTERIZE_FLOAT       y_segs  = y_max - y_min;

  //
  // inclusive subgroup scan of y_segs
  //
  SPN_RASTERIZE_FLOAT       y_iss   = spn_subgroup_scan_inclusive_add_float(y_segs);
  SPN_RASTERIZE_FLOAT       y_ess   = y_iss - y_segs;
  float                     y_rem   = spn_subgroup_last_float(y_iss);

  //
  // if this is a horizontal line then tweak y_iss so "is_scratch_store" always fails
  //
  if (y_segs == 0.0f)
    y_iss = 0.0f;

#if 0
  printf("{ { %5.0f, %5.0f }, { %5.0f, %5.0f } (* %5.0f / %5.0f / %5.0f / %5.0f *) }, \n",a0x,a0y,a1x,a1y,y_segs,y_iss,y_ess,y_rem);
#endif

  //
  // these values don't matter on first iteration
  //
  SPN_RASTERIZE_FLOAT n1x_prev = 0;
  SPN_RASTERIZE_FLOAT n1y_prev = 0;

  //
  // loop until done
  //
  while (y_rem > 0.0f)
    {
      //
      // distribute work across lanes
      //
      SPN_RASTERIZE_UINT const y_source = spn_scatter_scan_max(smem,y_iss,y_ess);

      //
      // get line at y_source line
      //
      const float m0x = spn_subgroup_shuffle(l0x,y_source);
      const float m0y = spn_subgroup_shuffle(l0y,y_source);
      const float m1x = spn_subgroup_shuffle(l1x,y_source);
      const float m1y = spn_subgroup_shuffle(l1y,y_source);

      //
      // every lane will create a 1 pixel tall line "sliver"
      //
      // FIXME -- this gets expanded on SIMD
      //
      // if numerator == 1 then this is the first lane
      // if numerator == s then this is the last  lane
      //
      SPN_RASTERIZE_FLOAT     const y_delta    = spn_delta_offset() - spn_subgroup_shuffle(y_ess,y_source);
      SPN_RASTERIZE_FLOAT     const y_count    = spn_subgroup_shuffle(y_segs,y_source);

      SPN_RASTERIZE_PREDICATE const is_y_first = (y_delta == 1.0f);
      SPN_RASTERIZE_PREDICATE const is_y_last  = (y_delta >= y_count);

      // toggle y_delta sign
      SPN_RASTERIZE_FLOAT     const y_offset   = as_float((as_uint(y_delta) ^ intel_sub_group_shuffle(dy_xor,y_source)));

      //
      // calculate "right" line segment endpoint
      //
      SPN_RASTERIZE_FLOAT       n1y = (y_offset + spn_subgroup_shuffle(y_base,y_source)) * SPN_SUBPIXEL_Y_SCALE_UP;
      const float n_t = (n1y - m0y) * spn_subgroup_shuffle(y_denom,y_source);
      SPN_RASTERIZE_FLOAT       n1x = round(SPN_LERP(m0x,m1x,n_t));

      //
      // override c1 if this is last point
      //
      n1y = select(n1y,m1y,is_y_last);
      n1x = select(n1x,m1x,is_y_last);

      //
      // shuffle up "left" line segment endpoint
      //
      // NOTE: Intel's shuffle_up is unique with its elegant
      // "previous" argument so don't get used to it
      //
      SPN_RASTERIZE_FLOAT n0y = spn_subgroup_shuffle_up_1(n1y_prev,n1y);
      SPN_RASTERIZE_FLOAT n0x = spn_subgroup_shuffle_up_1(n1x_prev,n1x);

      //
      // override shuffle up if this is the first line segment
      //
      n0y = select(n0y,m0y,is_y_first);
      n0x = select(n0x,m0x,is_y_first);

      //
      // save previous right endpoint
      //
      n1x_prev = n1x;
      n1y_prev = n1y;

      //
      // decrement by subgroup size
      //
      y_iss -= (float)SPN_RASTERIZE_ELEMS_PER_SUBGROUP;
      y_ess -= (float)SPN_RASTERIZE_ELEMS_PER_SUBGROUP;
      y_rem -= (float)SPN_RASTERIZE_ELEMS_PER_SUBGROUP;

#if 0
      //
      // debug
      //
      if (n0y != n1y) {
        printf("{ { %5.0f, %5.0f }, { %5.0f, %5.0f } },\n",n0x,n0y,n1x,n1y);
      }
#endif

      //
      // X-SLIVERING
      // -----------
      //
      // now sliver 1-pixel high lines into at either vertical or
      // 1-pixel wide lines
      //
      // save original direction and work with increasing x
      //
      SPN_RASTERIZE_PREDICATE const x_lt   = (n0x <= n1x);
      SPN_RASTERIZE_UINT      const dx_xor = x_lt ? 0 : 0x80000000;

      //
      // save 1/dy
      //
      const float x_denom  = native_recip(n1x - n0x);

      //
      // how many non-horizontal subpixel y-axis slivers are there?
      //
      const float x_min    = floor(fmin(n0x,n1x) * SPN_SUBPIXEL_X_SCALE_DOWN);
      const float x_max    = ceil (fmax(n0x,n1x) * SPN_SUBPIXEL_X_SCALE_DOWN);
      const float x_base   = x_lt ? x_min : x_max;
      const float x_segs   = fmax(x_max - x_min,1.0f);

      //
      // inclusive subgroup scan of y_segs
      //
      SPN_RASTERIZE_FLOAT       x_iss    = spn_subgroup_scan_inclusive_add_float(x_segs);
      SPN_RASTERIZE_FLOAT       x_ess    = x_iss - x_segs;
      float                     x_rem    = spn_subgroup_last_float(x_iss);

      //
      // if this is a horizontal line then tweak x_iss so "is_scratch_store" always fails
      //
      //if (x_segs == 0.0f)
      // x_iss = 0.0f;

      //
      // these values don't matter on first iteration
      //
      SPN_RASTERIZE_FLOAT       p1x_prev = 0;
      SPN_RASTERIZE_FLOAT       p1y_prev = 0;

      //
      // loop until done
      //
      while (x_rem > 0)
        {
          //
          // distribute work across lanes
          //
          SPN_RASTERIZE_UINT const x_source = spn_scatter_scan_max(smem,x_iss,x_ess);

          //
          // get line at y_source line
          //
          const float o0x = spn_subgroup_shuffle(n0x,x_source);
          const float o0y = spn_subgroup_shuffle(n0y,x_source);
          const float o1x = spn_subgroup_shuffle(n1x,x_source);
          const float o1y = spn_subgroup_shuffle(n1y,x_source);

          //
          // every lane will create a 1 pixel tall line "sliver"
          //
          // FIXME -- this gets expanded on SIMD
          //
          // if numerator == 1 then this is the first lane
          // if numerator == s then this is the last  lane
          //
          SPN_RASTERIZE_FLOAT     const x_delta    = spn_delta_offset() - spn_subgroup_shuffle(x_ess,x_source);
          SPN_RASTERIZE_FLOAT     const x_count    = spn_subgroup_shuffle(x_segs,x_source);

          SPN_RASTERIZE_PREDICATE const is_x_first = (x_delta == 1.0f);
          SPN_RASTERIZE_PREDICATE const is_x_last  = (x_delta >= x_count);

          // toggle x_delta sign
          SPN_RASTERIZE_FLOAT     const x_offset   = as_float((as_uint(x_delta) ^ intel_sub_group_shuffle(dx_xor,x_source)));

          //
          // calculate "right" line segment endpoint
          //
          SPN_RASTERIZE_FLOAT       p1x = (x_offset + spn_subgroup_shuffle(x_base,x_source)) * SPN_SUBPIXEL_X_SCALE_UP;
          const float p_t = (p1x - o0x) * spn_subgroup_shuffle(x_denom,x_source);
          SPN_RASTERIZE_FLOAT       p1y = round(SPN_LERP(o0y,o1y,p_t));

          //
          // override c1 if this is last point
          //
          p1x = select(p1x,o1x,is_x_last);
          p1y = select(p1y,o1y,is_x_last);

          //
          // shuffle up "left" line segment endpoint
          //
          // NOTE: Intel's shuffle_up is unique with its elegant
          // "previous" argument so don't get used to it
          //
          SPN_RASTERIZE_FLOAT p0x = spn_subgroup_shuffle_up_1(p1x_prev,p1x);
          SPN_RASTERIZE_FLOAT p0y = spn_subgroup_shuffle_up_1(p1y_prev,p1y);

          //
          // override shuffle up if this is the first line segment
          //
          p0x = select(p0x,o0x,is_x_first);
          p0y = select(p0y,o0y,is_x_first);

          //
          // save previous right endpoint
          //
          p1x_prev = p1x;
          p1y_prev = p1y;

          //
          // decrement by subgroup size
          //
          x_iss -= SPN_RASTERIZE_ELEMS_PER_SUBGROUP;
          x_ess -= SPN_RASTERIZE_ELEMS_PER_SUBGROUP;
          x_rem -= SPN_RASTERIZE_ELEMS_PER_SUBGROUP;

          //
          // only non-horizontal subpixel lines are valid
          //
          SPN_RASTERIZE_PREDICATE is_active = (p0y != p1y);

          //
          // if no lanes are active then continue
          //
          // FIXME -- THIS SIMPLE SUB_GROUP_ANY TEST SIGNIFICANTLY
          // IMPACTS PERFORMANCE (+12% ?)
          //
          // IT SHOULDN'T !!!
          //
#if 0
          if (!spn_subgroup_any(is_active))
            continue;
#endif

          //
          // Option 1: use SLM for explicitly managed coalesced stores
          //
          // 1. which tile does this line belong?
          // 2. hash tile coordinates
          // 3. lookup hash
          // 4. if tile matches then SLM append keys
          // 5. if tile doesn't match
          //   a. flush
          //   b. create new TTSK_RYX
          //   c. obtain TTSB block from pool
          //   d. goto 3.
          //

          //
          // Option 2: rely on L1/L2/L3 to mitigate non-coalesced stores
          //
          // 1. which tile does this line belong?
          // 2. hash tile coordinates
          // 3. lookup hash
          // 4. if tile matches then GMEM append keys
          // 5. if tile doesn't match
          //   a. flush (and invalidate empty elems)
          //   b. create new TTSK_RYX
          //   c. obtain TTSB block from pool
          //   d. goto 3.
          //

          //
          // The virtual rasterization surface is very large and
          // signed: +/- ~64K-256K, depending on the architecture.
          //
          // Rasters must be clipped to the virtual surface and,
          // optionally, clipped even further on a per raster
          // basis.
          //

          //
          // Clip to the per-raster clip
          //

          /*

            CLIP HERE

          */

          //
          // Hash the tile coordinates
          //
          // This table lists nominal values for each architecture.
          // We want to choose values that are naturally fit the
          // "width" of the architecture.
          //
          //   SIMD   RANGE   BITS  MAX RANGE  MAX BINS  HASH BITS
          //   ----  -------  ----  ---------  --------  ---------
          //     4   [0,  4]    3    [0,  7]      10      mod(10)  <-- SSE42, ?
          //     8   [0,  8]    4    [0, 15]       8         3     <-- GEN*,AVX*
          //    16   [0, 16]    5    [0, 31]       6      mod(6)   <-- GEN*,?
          //    32   [0, 32]    6    [0, 63]       5      mod(5)   <-- CUDA,PowerVR,Adreno,GEN*
          //    64   [0, 64]    7    [0,127]       4         2     <-- AMD Radeon
          //
          // NOTE: When possible, bias the hash toward using more y
          // bits because of:
          //
          //   1. the 90 degree counter-clockwise rotation that we put
          //      in place to offset the render-time clockwise
          //      rotation
          //
          //   2. the likely presence of left-to-right or
          //      right-to-left glyphs.
          //
          // For power-of-two bins, the hash is easy.
          //
          // For non-power-of-two, we may want to either implement a
          // fast mod (compiler should do this for us... hahahaha) or
          // drop down to the next power-of-two.
          //

          //
          // FIXME -- this snarl is not good -- can probably reduce
          // some of the sign casting but some is there to vectorize a
          // scalar
          //
          SPN_RASTERIZE_INT       const z0y    = SPN_CONVERT(SPN_RASTERIZE_INT)(p0y);
          SPN_RASTERIZE_INT       const z1y    = SPN_CONVERT(SPN_RASTERIZE_INT)(p1y);

          SPN_RASTERIZE_INT       const z0x    = SPN_CONVERT(SPN_RASTERIZE_INT)(p0x);
          SPN_RASTERIZE_INT       const z1x    = SPN_CONVERT(SPN_RASTERIZE_INT)(p1x);

          SPN_RASTERIZE_INT       const min_y  = min(z0y,z1y);
          SPN_RASTERIZE_INT       const max_y  = max(z0y,z1y);

          SPN_RASTERIZE_INT       const tile_y = min_y >> SPN_SUBTILE_RESL_Y_LOG2;

          SPN_RASTERIZE_UINT      const ty     = SPN_AS(SPN_RASTERIZE_UINT)(min_y) & SPN_SUBTILE_MASK_Y;
          SPN_RASTERIZE_INT             dy     = SPN_AS(SPN_RASTERIZE_INT)(z1y - z0y);

          //
          // map [+1,+32] to [ 0,+31]
          // map [-1,-32] to [-1,-32]
          //
          SPN_RASTERIZE_INT             dys    = (dy + (~dy >> 31)) << 26;

          SPN_RASTERIZE_INT       const min_x  = min(z0x,z1x);
          SPN_RASTERIZE_INT       const max_x  = max(z0x,z1x);
          SPN_RASTERIZE_INT       const tile_x = min_x >> SPN_SUBTILE_RESL_X_LOG2;

          SPN_RASTERIZE_UINT      const tx     = SPN_AS(SPN_RASTERIZE_UINT)(min_x) & SPN_SUBTILE_MASK_X;
          SPN_RASTERIZE_UINT      const sx     = SPN_AS(SPN_RASTERIZE_UINT)(max_x - min_x);

          SPN_RASTERIZE_UINT      const tts    = dys | (ty << 16) | (sx << 10) | tx;

          SPN_RASTERIZE_UINT      const hash   = (((SPN_AS(SPN_RASTERIZE_UINT)(tile_y) & SPN_RASTERIZE_TILE_HASH_Y_MASK) << SPN_RASTERIZE_TILE_HASH_X_BITS) |
                                                   (SPN_AS(SPN_RASTERIZE_UINT)(tile_x) & SPN_RASTERIZE_TILE_HASH_X_MASK));

          SPN_RASTERIZE_UINT      const yx     = (((SPN_AS(SPN_RASTERIZE_UINT)(tile_y) & 0xFFF) << 12) | (SPN_AS(SPN_RASTERIZE_UINT)(tile_x) & 0xFFF));

#if 0
          printf("(%3u, %3u)\n",tile_y,tile_x);
#endif

#if 0
          if (is_active)
            printf("( %3u, %3u ) : [ %3u, %3u, %3d, %3d, %3u ]\n",tile_y,tile_x,ty,tx,dy,((int)dys)>>26,sx);
#endif

          //
          // debug
          //
#if 0 // PRINTF_ENABLE

#if ( SPN_RASTERIZE_SUBGROUP_SIZE == 1 )

#undef  SPN_EXPAND_X
#define SPN_EXPAND_X(I,S,C,P,A)                                         \
          if (is_active C)                                              \
            printf("{ { %5d, %5d }, { %5d, %5d } (* %2u *) },\n",z0x C,z0y C,z1x C,z1y C,hash C);

          SPN_RASTERIZE_VECTOR_EXPAND();
#else
          if (is_active)
            printf("{ { %5d, %5d }, { %5d, %5d } } (* %2u *),\n",z0x,z0y,z1x,z1y,hash);
#endif

#endif
          //
          // flush all active lanes
          //
          while (true)
            {
              //
              // either gather load or vector load+shuffle the yx keys
              //
#if ( SPN_RASTERIZE_SUBGROUP_SIZE == 1 )
              SPN_RASTERIZE_BIN       const yx_bin     = smem->bin.vN.yx;
              SPN_RASTERIZE_UINT      const yx_cur     = shuffle(yx_bin,hash);
#else
              SPN_RASTERIZE_UINT      const yx_cur     = smem->bin.aN.yx[hash];
#endif

              //
              // does yx for lane match yx for hash?
              //
              SPN_RASTERIZE_UINT      const active_yx  = is_active ? yx : SPN_RASTERIZE_YX_INVALID;
              SPN_RASTERIZE_PREDICATE const is_match   = (yx_cur == active_yx);

              //
              // OpenCL spec: "When casting a bool to a vector integer
              // data type, the vector components will be set to -1
              // (i.e. all bits set) if the vector bool value is true
              // and 0 otherwise.
              //
#if ( SPN_RASTERIZE_VECTOR_SIZE_LOG2 == 0 )
              SPN_RASTERIZE_UINT      const h_match    = (SPN_RASTERIZE_UINT)is_match;
#else
              SPN_RASTERIZE_UINT      const h_match    = abs(is_match); // {-1,0} -> {+1,0}
#endif
              //
              // how many new elements for each matching hash bin?
              //
              SPN_RASTERIZE_UINT      const h_shl      = hash * SPN_RASTERIZE_TILE_HASH_BIN_BITS;
              SPN_RASTERIZE_UINT      const h          = h_match << h_shl;

              //
              // prefix sum all of the bins in parallel
              //
              SPN_RASTERIZE_UINT      const h_iss      = spn_subgroup_scan_inclusive_add_uint(h);
              SPN_RASTERIZE_UINT      const h_total    = spn_subgroup_last_uint(h_iss);

              //
              // current bin counts
              //
#if ( SPN_RASTERIZE_SUBGROUP_SIZE == 1 )
              SPN_RASTERIZE_BIN       const count_bin  = smem->bin.vN.count;
              SPN_RASTERIZE_UINT      const count_cur  = shuffle(count_bin,hash);
#else
              SPN_RASTERIZE_UINT      const count_cur  = smem->bin.aN.count[hash];
#endif

              //
              // calculate where each cache-hit and in-bounds tts should be stored
              //
              SPN_RASTERIZE_UINT      const ttsb_index = (h_iss   >> h_shl & SPN_RASTERIZE_TILE_HASH_BIN_MASK) + count_cur - 1;
              SPN_RASTERIZE_UINT      const count_new  = (h_total >> h_shl & SPN_RASTERIZE_TILE_HASH_BIN_MASK) + count_cur;

              //
              // which lanes can append to a matching bin?
              //
              SPN_RASTERIZE_PREDICATE const is_append  = is_match && (ttsb_index < SPN_DEVICE_SUBBLOCK_WORDS);

              //
              // scatter append tts elements to bin blocks
              //
#if ( SPN_RASTERIZE_SUBGROUP_SIZE == 1)
              //
              // SIMD
              //
#undef  SPN_EXPAND_X
#define SPN_EXPAND_X(I,S,C,P,A)                                         \
              if (is_append C)                                          \
                {                                                       \
                  smem->bin.aN.ttsb [hash C][ttsb_index C] = tts       C; \
                  smem->bin.aN.count[hash C]               = count_new C; \
                }

              SPN_RASTERIZE_VECTOR_EXPAND();
#else
              //
              // SIMT
              //
              if (is_append)
                {
                  smem->bin.aN.ttsb [hash][ttsb_index] = tts;
                  smem->bin.aN.count[hash]             = count_new; // it's ok if this is > SPN_DEVICE_SUBBLOCK_WORDS
                }
#endif
              //
              // try to keep predicate updates SIMD-friendly and
              // outside of predicated code paths -- this is not
              // always how we would normally do things on SIMT but
              // either approach is acceptable
              //

              //
              // mask off lanes/components that successfully appended
              //
              is_active = is_active && !is_append;

              //
              // are there any active lanes left?
              //
              if (!spn_subgroup_any(is_active))
                break;

              //
              // There are active lanes that couldn't be appended to a
              // bin because their hashes collided with the bin's
              // current ryx key then those bins must be ejected.
              //
              // Note that we do not eject "full" bins because lazily
              // waiting for a collision results in simpler code.
              //
              spn_flush(bp_atomics,
                        bp_blocks,
                        bp_ids,
                        bp_mask,
                        cohort_atomics,
                        subblocks,
                        blocks,
                        blocks_next,
                        sk_v,
                        sk_v_next,
                        sk_extent,
                        smem,
                        hash,
                        yx,
                        is_active);
            }
        }
    }
}

//
// INITIALIZE SMEM
//
// Note that SIMD/SIMT have nearly the same syntax.
//
static
void
spn_smem_init(__local struct spn_subgroup_smem volatile * const smem)
{
  //
  // initialize smem bins
  //
#if ( SPN_RASTERIZE_SUBGROUP_SIZE == 1 )
  //
  // SIMD
  //
  smem->bin.vN.yx    = ( SPN_RASTERIZE_YX_INIT );
  smem->bin.vN.count = ( 0 );
#else
  //
  // SIMT
  //
  int idx = spn_subgroup_lane();

#if   ( SPN_RASTERIZE_TILE_HASH_BIN_COUNT < SPN_RASTERIZE_ELEMS_PER_SUBGROUP )
  if (idx < SPN_RASTERIZE_TILE_HASH_BIN_COUNT)
#elif ( SPN_RASTERIZE_TILE_HASH_BIN_COUNT > SPN_RASTERIZE_ELEMS_PER_SUBGROUP )
  for (; idx<SPN_RASTERIZE_TILE_HASH_BIN_COUNT; idx+=SPN_RASTERIZE_SUBGROUP_SIZE)
#endif
    {
      smem->bin.aN.yx   [idx] = ( SPN_RASTERIZE_YX_INIT );
      smem->bin.aN.count[idx] = ( 0 );
    }
#endif
}

//
// RASTERIZE CUBIC KERNEL
//

static
void
spn_rasterize_cubics(__global SPN_ATOMIC_UINT         volatile * const bp_atomics,
                     __global union spn_bp_elem                * const bp_blocks,
                     __global uint                             * const bp_ids,
                     spn_uint                                    const bp_mask,

                     __global SPN_ATOMIC_UINT         volatile * const cohort_atomics,
                     __global spn_ttsk_s_t                     * const sk_extent,

                     __local struct spn_subgroup_smem volatile * const smem,

                     spn_uint                                  * const nodeword,
                     spn_block_id_t                            * const id,

                     union spn_transform              const    * const tv,
                     union spn_path_clip              const    * const cv,
                     spn_uint                                    const cohort)
{
  //
  // the initial segment idx and segments-per-block constant determine
  // how many block ids will need to be loaded
  //
  const float c0x = bp_blocks[SPN_RASTERIZE_SEGMENT(*id)].coord;

  spn_segment_next(bp_blocks,nodeword,id);

  const float c0y = bp_blocks[SPN_RASTERIZE_SEGMENT(*id)].coord;

  spn_segment_next(bp_blocks,nodeword,id);

  const float c1x = bp_blocks[SPN_RASTERIZE_SEGMENT(*id)].coord;

  spn_segment_next(bp_blocks,nodeword,id);

  const float c1y = bp_blocks[SPN_RASTERIZE_SEGMENT(*id)].coord;

  spn_segment_next(bp_blocks,nodeword,id);

  const float c2x = bp_blocks[SPN_RASTERIZE_SEGMENT(*id)].coord;

  spn_segment_next(bp_blocks,nodeword,id);

  const float c2y = bp_blocks[SPN_RASTERIZE_SEGMENT(*id)].coord;

  spn_segment_next(bp_blocks,nodeword,id);

  const float c3x = bp_blocks[SPN_RASTERIZE_SEGMENT(*id)].coord;

  spn_segment_next(bp_blocks,nodeword,id);

  const float c3y = bp_blocks[SPN_RASTERIZE_SEGMENT(*id)].coord;

  //
  // apply transform
  //
  // note that we only care if the end points are rounded to subpixel precision
  //
  // FIXME -- transformation is currently affine-only support perspective later
  //
  // the affine transformation requires 8 FMA + 2 ROUND operations
  //
  const float b0x = round(c0x * tv->sx  + c0y * tv->shx + tv->tx);
  const float b0y = round(c0x * tv->shy + c0y * tv->sy  + tv->ty);

  const float t1x = c1x * tv->sx  + c1y * tv->shx + tv->tx;
  const float t1y = c1x * tv->shy + c1y * tv->sy  + tv->ty;

  const float t2x = c2x * tv->sx  + c2y * tv->shx + tv->tx;
  const float t2y = c2x * tv->shy + c2y * tv->sy  + tv->ty;

  const float t3x = round(c3x * tv->sx  + c3y * tv->shx + tv->tx);
  const float t3y = round(c3x * tv->shy + c3y * tv->sy  + tv->ty);

  //
  //
  //
#if PRINTF_ENABLE

#if ( SPN_RASTERIZE_SUBGROUP_SIZE == 1 )

#undef  SPN_EXPAND_X
#define SPN_EXPAND_X(I,S,C,P,A)                                         \
  printf("{ { %.02f, %.02f }, { %.02f, %.02f },"                        \
         "  { %.02f, %.02f }, { %.02f, %.02f } },\n",                   \
         b0x C,b0y C,t1x C,t1y C,                                       \
         t2x C,t2y C,t3x C,t3y C);

  SPN_RASTERIZE_VECTOR_EXPAND();

#else

  printf("{ { %.02f, %.02f }, { %.02f, %.02f }, { %.02f, %.02f }, { %.02f, %.02f } },\n",
         b0x,b0y,t1x,t1y,t2x,t2y,t3x,t3y);

#endif

#endif

  //
  // OLD APPROACH
  // ------------
  //
  // The Spinel CUDA rasterizer was significantly more complex and
  // performed a few different tasks that are probably best kept
  // separate.
  //
  // The Spinel rasterizer Bezier held 4-element x and y coordinates
  // in adjacent lanes. This simplified intermingling of single lane
  // 4-coordinate line segments with two-lane cubic Beziers.
  //
  // After transformation of the input segments, the Spinel rasterizer
  // would test cubics for flatness and, if flat, collapse the
  // adjacent lanes into a single line lane and an empty lane.
  //
  // Any lines would then be appended to a line queue.
  //
  // Any cubics would then be subdivided.
  //
  // The reclassification process would be repeated.
  //
  // NEW APPROACH
  // ------------
  //
  // Assume we're only working with cubics in this kernel.
  //
  // Optimization: if the line segment is a special case -- a cusp,
  // has 1+ inflections, or a loop -- it might be beneficial to
  // subdivide the control cage 1+ times in order to separate the
  // flatter segments the high-velocity region(s).
  //
  // This means we want to split using [a,b] formulation to _directly_
  // subdivide producing a new control cage.
  //
  // Wang's Formula is still useful even if we subdivide once or twice
  // as it's so cheap that it might give some useful hints about where
  // the high-velocity sections of curve reside.
  //
  // But it seems like using Wang's and directly flattening to line
  // segments without any subdivision is good enough for the limited
  // set of test cases that I've tried.
  //
  // So... use Wang's Formula to estimate how many line segment are
  // required to properly flatten the cubics.
  //
  // Then use inclusive/exclusive scans to put all the lanes to work:
  //
  //   1. segmenting cubics to line segments
  //
  //   2. slivering line segments into 1-pixel high line segments
  //
  //   3. slivering 1-pixel high line segments into 1-pixel wide line
  //      segments
  //
  // MORE BACKGROUND ON NEW APPROACH
  // -------------------------------
  //
  // Two options for handling line segments:
  //
  // 1. append the line segments onto an SLM array until enough
  //    work has been accrued (Spinel does this)
  //
  // 2. immediately sliver the potentially multi-pixel line
  //    segments into subpixel lines
  //
  // The advantage of (1) is that it guarantees the slivering
  // process will, on average, always be emitting a full subgroup
  // of subpixel lines.
  //
  // The advantage of (2) is that it reduces code complexity and
  // leaves more room for SLM tile bins. The difference between Spinel
  // and Skia Compute is that Wang's Formula guarantees there will be
  // a full subgroup of multi-pixel lines unless this is the final
  // iteration of the warp of multi-pixel lines.
  //
  // Note that wider GPU architectures might benefit from (1) and
  // other work accumulation strategies because it will minimize
  // partial warp workloads in the final iteration of each stage.  It
  // also minimizes the sunk cost of the uniform control logic steps.
  //
  // So let's implement (2) for now...
  //

  //
  // And... begin!
  //
  // Estimate how many line segments are in quad/cubic curve.
  //
  // Wang's Formula will return zero if the control points are
  // collinear but we bump it up to 1.0f.
  //
  const float s_segs  = spn_wangs_formula_cubic(b0x,b0y,t1x,t1y,t2x,t2y,t3x,t3y);

  //
  // if there are free registers then precalculate the reciprocal for
  // each estimated segments since it will never change
  //
  const float s_denom = native_recip(s_segs);


  //
  // inclusive add scan of estimated line segments
  // exclusive add scan of estimated line segments
  // total number       of estimated line segments
  //
  SPN_RASTERIZE_FLOAT       s_iss   = spn_subgroup_scan_inclusive_add_float(s_segs);
  SPN_RASTERIZE_FLOAT       s_ess   = s_iss - s_segs;
  float                     s_rem   = spn_subgroup_last_float(s_iss); // scalar

  //
  // Precompute cubic polynomial coefficients from transformed control
  // cage so we can shuffle them in on each iteration of the outer
  // loop and then evaluate the polynomial in Horner form.
  //
  //                            |  1  0  0  0 | | c0 |
  //                            |             | |    |
  //                            | -3  3  0  0 | | c1 |
  //   B(t) = [ 1 t^1 t^2 t^3 ] |             | |    |
  //                            |  3 -6  3  0 | | c2 |
  //                            |             | |    |
  //                            | -1  3 -3  1 | | c3 |
  //
  //
  const float b1x = mad(-3.0f,b0x,3.0f*t1x);                // 2 - 1 MAD + MUL
  const float b1y = mad(-3.0f,b0y,3.0f*t1y);                // 2 - 1 MAD + MUL

  const float b2x = mad(3.0f,b0x,mad(-6.0f,t1x,3.0f*t2x));  // 3 - 2 MAD + MUL
  const float b2y = mad(3.0f,b0y,mad(-6.0f,t1y,3.0f*t2y));  // 3 - 2 MAD + MUL

  const float b3x = mad(3.0f,t1x,mad(-3.0f,t2x,t3x)) - b0x; // 3 - 2 MAD + SUB
  const float b3y = mad(3.0f,t1y,mad(-3.0f,t2y,t3y)) - b0y; // 3 - 2 MAD + SUB

  //
  // these values don't matter on the first iteration
  //
  SPN_RASTERIZE_FLOAT l1x_prev  = 0;
  SPN_RASTERIZE_FLOAT l1y_prev  = 0;

  //
  // allocate and init in-register TTSK keys
  //
  spn_uint     sk_v_next = 0;
  spn_ttsk_v_t sk_v;

  sk_v.hi = cohort;

  //
  // initialize smem
  //
  spn_smem_init(smem);

  //
  // initialize blocks / subblocks
  //
  spn_block_id_v_t blocks;
  spn_uint         blocks_next = SPN_RASTERIZE_BLOCK_ID_V_SIZE;

#if SPN_DEVICE_BLOCK_WORDS_LOG2 > SPN_DEVICE_SUBBLOCK_WORDS_LOG2
  spn_block_id_t   subblocks   = 0;
#endif

  //
  // loop until done
  //
  while (s_rem > 0)
    {
      //
      // distribute work across lanes
      //
      SPN_RASTERIZE_UINT const s_source = spn_scatter_scan_max(smem,s_iss,s_ess);

      //
      // every lane has a fraction to work off of
      //
      // FIXME -- this gets expanded on SIMD
      //
      // if delta == 1      then this is the first lane
      // if count == s_segs then this is the last  lane
      //
      SPN_RASTERIZE_FLOAT     const s_delta    = spn_delta_offset() - spn_subgroup_shuffle(s_ess,s_source);
      SPN_RASTERIZE_FLOAT     const s_count    = spn_subgroup_shuffle(s_segs,s_source);

      SPN_RASTERIZE_PREDICATE const is_s_first = (s_delta == 1.0f);
      SPN_RASTERIZE_PREDICATE const is_s_last  = (s_delta >= s_count);

      //
      // init parametric t
      //
      SPN_RASTERIZE_FLOAT s_t = s_delta * spn_subgroup_shuffle(s_denom,s_source); // faster than native_recip(s_count)?

      //
      // if last then override to a hard 1.0f
      //
      s_t    = is_s_last ? 1.0f : s_t;

      //
      // decrement by subgroup size
      //
      s_iss -= SPN_RASTERIZE_ELEMS_PER_SUBGROUP;
      s_ess -= SPN_RASTERIZE_ELEMS_PER_SUBGROUP;
      s_rem -= SPN_RASTERIZE_ELEMS_PER_SUBGROUP;

      //
      // now every lane knows what to do and the following lines will
      // pump out up to SUBGROUP_SIZE line segments
      //
      // obtain the src vertices through shared or via a shuffle
      //

      //
      // shuffle in the polynomial coefficients their source lane
      //
      const float s0x = spn_subgroup_shuffle(b0x,s_source);
      const float s0y = spn_subgroup_shuffle(b0y,s_source);

      const float s1x = spn_subgroup_shuffle(b1x,s_source);
      const float s1y = spn_subgroup_shuffle(b1y,s_source);

      const float s2x = spn_subgroup_shuffle(b2x,s_source);
      const float s2y = spn_subgroup_shuffle(b2y,s_source);

      const float s3x = spn_subgroup_shuffle(b3x,s_source);
      const float s3y = spn_subgroup_shuffle(b3y,s_source);

      //
      // calculate "right" line segment endpoint using Horner form
      //
      SPN_RASTERIZE_FLOAT       l1x = round(mad(mad(mad(s3x,s_t,s2x),s_t,s1x),s_t,s0x)); // 3 MAD + ROUND
      SPN_RASTERIZE_FLOAT       l1y = round(mad(mad(mad(s3y,s_t,s2y),s_t,s1y),s_t,s0y)); // 3 MAD + ROUND

      //
      // shuffle up "left" line segment endpoint
      //
      // NOTE: Intel's shuffle_up is unique with its elegant
      // "previous" argument so don't get used to it
      //
      SPN_RASTERIZE_FLOAT       l0x = spn_subgroup_shuffle_up_1(l1x_prev,l1x);
      SPN_RASTERIZE_FLOAT       l0y = spn_subgroup_shuffle_up_1(l1y_prev,l1y);

      //
      // save previous right endpoint
      //
      l1x_prev = l1x;
      l1y_prev = l1y;

      //
      // override shuffle up if this is the first line segment
      //
      l0x = select(l0x,s0x,is_s_first);
      l0y = select(l0y,s0y,is_s_first);

      //
      // sliver lines
      //
      spn_sliver(bp_atomics,
                 bp_blocks,
                 bp_ids,
                 bp_mask,
                 cohort_atomics,
                 &subblocks,
                 &blocks,
                 &blocks_next,
                 &sk_v,
                 &sk_v_next,
                 sk_extent,
                 smem,
                 l0x,l0y,l1x,l1y);
    }

  //
  // - flush work-in-progress blocks
  // - return unused block ids
  //
  spn_finalize(bp_atomics,
               bp_blocks,
               bp_ids,
               bp_mask,
               cohort_atomics,
               &blocks,
               blocks_next,
               &sk_v,
               sk_v_next,
               sk_extent,
               smem);
}

//
// RASTERIZE QUAD KERNEL
//

static
void
spn_rasterize_quads(__global SPN_ATOMIC_UINT         volatile * const bp_atomics,
                    __global union spn_bp_elem                * const bp_blocks,
                    __global uint                             * const bp_ids,
                    spn_uint                                    const bp_mask,

                    __global SPN_ATOMIC_UINT         volatile * const cohort_atomics,
                    __global spn_ttsk_s_t                     * const sk_extent,

                    __local struct spn_subgroup_smem volatile * const smem,

                    spn_uint                                  * const nodeword,
                    spn_block_id_t                            * const id,

                    union spn_transform              const    * const tv,
                    union spn_path_clip              const    * const cv,
                    spn_uint                                    const cohort)
{
  //
  // the initial segment idx and segments-per-block constant determine
  // how many block ids will need to be loaded
  //
  const float c0x = bp_blocks[SPN_RASTERIZE_SEGMENT(*id)].coord;

  spn_segment_next(bp_blocks,nodeword,id);

  const float c0y = bp_blocks[SPN_RASTERIZE_SEGMENT(*id)].coord;

  spn_segment_next(bp_blocks,nodeword,id);

  const float c1x = bp_blocks[SPN_RASTERIZE_SEGMENT(*id)].coord;

  spn_segment_next(bp_blocks,nodeword,id);

  const float c1y = bp_blocks[SPN_RASTERIZE_SEGMENT(*id)].coord;

  spn_segment_next(bp_blocks,nodeword,id);

  const float c2x = bp_blocks[SPN_RASTERIZE_SEGMENT(*id)].coord;

  spn_segment_next(bp_blocks,nodeword,id);

  const float c2y = bp_blocks[SPN_RASTERIZE_SEGMENT(*id)].coord;

  //
  // apply transform
  //
  // note that we only care if the end points are rounded to subpixel precision
  //
  // FIXME -- transformation is currently affine-only support perspective later
  //
  // the affine transformation requires 8 FMA + 2 ROUND operations
  //
  const float b0x = round(c0x * tv->sx  + c0y * tv->shx + tv->tx);
  const float b0y = round(c0x * tv->shy + c0y * tv->sy  + tv->ty);

  const float t1x = c1x * tv->sx  + c1y * tv->shx + tv->tx;
  const float t1y = c1x * tv->shy + c1y * tv->sy  + tv->ty;

  const float t2x = round(c2x * tv->sx  + c2y * tv->shx + tv->tx);
  const float t2y = round(c2x * tv->shy + c2y * tv->sy  + tv->ty);

  //
  // Estimate how many line segments are in quad/cubic curve.
  //
  // Wang's Formula will return zero if the control points are
  // collinear but we bump it up to 1.0f.
  //
  const float s_segs  = spn_wangs_formula_quadratic(b0x,b0y,t1x,t1y,t2x,t2y);

  //
  // if there are free registers then precalculate the reciprocal for
  // each estimated segments since it will never change
  //
  const float s_denom = native_recip(s_segs);


  //
  // inclusive add scan of estimated line segments
  // exclusive add scan of estimated line segments
  // total number       of estimated line segments
  //
  SPN_RASTERIZE_FLOAT       s_iss   = spn_subgroup_scan_inclusive_add_float(s_segs);
  SPN_RASTERIZE_FLOAT       s_ess   = s_iss - s_segs;
  float                     s_rem   = spn_subgroup_last_float(s_iss); // scalar

  //
  // Precompute quadratic polynomial coefficients from control cage so
  // we can shuffle them in on each iteration of the outer loop and
  // then evaluate the polynomial in Horner form.
  //

  //                        |  1  0  0  | | c0 |
  //                        |           | |    |
  //   B(t) = [ 1 t^1 t^2 ] | -2  2  0  | | c1 |
  //                        |           | |    |
  //                        |  1 -2  1  | | c2 |
  //
  //
  const float b1x = mad(-2.0f,b0x,2.0f*t1x); // 2 - 1 MAD + MUL
  const float b1y = mad(-2.0f,b0y,2.0f*t1y); // 2 - 1 MAD + MUL

  const float b2x = mad(-2.0f,t1x,b0x+t2x);  // 2 - 1 MAD + ADD
  const float b2y = mad(-2.0f,t1y,b0y+t2y);  // 2 - 1 MAD + ADD

  //
  // these values don't matter on the first iteration
  //
  SPN_RASTERIZE_FLOAT l1x_prev  = 0;
  SPN_RASTERIZE_FLOAT l1y_prev  = 0;

  //
  // allocate and init in-register TTSK keys
  //
  spn_uint     sk_v_next = 0;
  spn_ttsk_v_t sk_v;

  sk_v.hi = cohort;

  //
  // initialize smem
  //
  spn_smem_init(smem);

  //
  // initialize blocks / subblocks
  //
  spn_block_id_v_t blocks;
  spn_uint         blocks_next = SPN_RASTERIZE_BLOCK_ID_V_SIZE;

#if SPN_DEVICE_BLOCK_WORDS_LOG2 > SPN_DEVICE_SUBBLOCK_WORDS_LOG2
  spn_block_id_t   subblocks   = 0;
#endif

  //
  // loop until done
  //
  while (s_rem > 0)
    {
      //
      // distribute work across lanes
      //
      SPN_RASTERIZE_UINT const s_source = spn_scatter_scan_max(smem,s_iss,s_ess);

      //
      // every lane has a fraction to work off of
      //
      // FIXME -- this gets expanded on SIMD
      //
      // if delta == 1      then this is the first lane
      // if count == s_segs then this is the last  lane
      //
      SPN_RASTERIZE_FLOAT     const s_delta    = spn_delta_offset() - spn_subgroup_shuffle(s_ess,s_source);
      SPN_RASTERIZE_FLOAT     const s_count    = spn_subgroup_shuffle(s_segs,s_source);

      SPN_RASTERIZE_PREDICATE const is_s_first = (s_delta == 1.0f);
      SPN_RASTERIZE_PREDICATE const is_s_last  = (s_delta >= s_count);

      //
      // init parametric t
      //
      SPN_RASTERIZE_FLOAT s_t = s_delta * spn_subgroup_shuffle(s_denom,s_source); // faster than native_recip(s_count)?

      //
      // if last then override to a hard 1.0f
      //
      s_t    = is_s_last ? 1.0f : s_t;

      //
      // decrement by subgroup size
      //
      s_iss -= SPN_RASTERIZE_ELEMS_PER_SUBGROUP;
      s_ess -= SPN_RASTERIZE_ELEMS_PER_SUBGROUP;
      s_rem -= SPN_RASTERIZE_ELEMS_PER_SUBGROUP;

      //
      // now every lane knows what to do and the following lines will
      // pump out up to SUBGROUP_SIZE line segments
      //
      // obtain the src vertices through shared or via a shuffle
      //

      //
      // shuffle in the polynomial coefficients their source lane
      //
      const float s0x = spn_subgroup_shuffle(b0x,s_source);
      const float s0y = spn_subgroup_shuffle(b0y,s_source);

      const float s1x = spn_subgroup_shuffle(b1x,s_source);
      const float s1y = spn_subgroup_shuffle(b1y,s_source);

      const float s2x = spn_subgroup_shuffle(b2x,s_source);
      const float s2y = spn_subgroup_shuffle(b2y,s_source);

      //
      // calculate "right" line segment endpoint using Horner form
      //
      SPN_RASTERIZE_FLOAT       l1x = round(mad(mad(s2x,s_t,s1x),s_t,s0x)); // 2 MAD + ROUND
      SPN_RASTERIZE_FLOAT       l1y = round(mad(mad(s2y,s_t,s1y),s_t,s0y)); // 2 MAD + ROUND

      //
      // shuffle up "left" line segment endpoint
      //
      // NOTE: Intel's shuffle_up is unique with its elegant
      // "previous" argument so don't get used to it
      //
      SPN_RASTERIZE_FLOAT       l0x = spn_subgroup_shuffle_up_1(l1x_prev,l1x);
      SPN_RASTERIZE_FLOAT       l0y = spn_subgroup_shuffle_up_1(l1y_prev,l1y);

      //
      // save previous right endpoint
      //
      l1x_prev = l1x;
      l1y_prev = l1y;

      //
      // override shuffle up if this is the first line segment
      //
      l0x = select(l0x,s0x,is_s_first);
      l0y = select(l0y,s0y,is_s_first);

      //
      // sliver lines
      //
      spn_sliver(bp_atomics,
                 bp_blocks,
                 bp_ids,
                 bp_mask,
                 cohort_atomics,
                 &subblocks,
                 &blocks,
                 &blocks_next,
                 &sk_v,
                 &sk_v_next,
                 sk_extent,
                 smem,
                 l0x,l0y,l1x,l1y);
    }

  //
  // - flush work-in-progress blocks
  // - return unused block ids
  //
  spn_finalize(bp_atomics,
               bp_blocks,
               bp_ids,
               bp_mask,
               cohort_atomics,
               &blocks,
               blocks_next,
               &sk_v,
               sk_v_next,
               sk_extent,
               smem);
}

//
// RASTERIZE LINE KERNEL
//

static
void
spn_rasterize_lines(__global SPN_ATOMIC_UINT         volatile * const bp_atomics,
                    __global union spn_bp_elem                * const bp_blocks,
                    __global uint                             * const bp_ids,
                    spn_uint                                    const bp_mask,

                    __global SPN_ATOMIC_UINT         volatile * const cohort_atomics,
                    __global spn_ttsk_s_t                     * const sk_extent,

                    __local struct spn_subgroup_smem volatile * const smem,

                    spn_uint                                  * const nodeword,
                    spn_block_id_t                            * const id,

                    union spn_transform              const    * const tv,
                    union spn_path_clip              const    * const cv,
                    spn_uint                                    const cohort)
{
}

//
//
//

__kernel
SPN_RASTERIZE_KERNEL_ATTRIBS
void
spn_kernel_rasterize_all(__global SPN_ATOMIC_UINT         volatile * const bp_atomics,
                         __global union spn_bp_elem                * const bp_blocks,
                         __global uint                             * const bp_ids,
                         spn_uint                                    const bp_mask,

                         __global SPN_ATOMIC_UINT         volatile * const cohort_atomics,
                         __global spn_ttsk_s_t                     * const sk_extent,

                         __global float8                  const    * const transforms, // FIXME -- __constant
                         __global float4                  const    * const clips,      // FIXME -- __constant
                         __global union spn_cmd_rasterize const    * const cmds,       // FIXME -- __constant
                         spn_uint                                    const count)
{
  //
  // declare shared memory block
  //
#if ( SPN_RASTERIZE_WORKGROUP_SUBGROUPS == 1 )
  __local struct spn_subgroup_smem volatile                smem[1];
#else
  __local struct spn_subgroup_smem volatile                smem_wg[SPN_RASTERIZE_WORKGROUP_SUBGROUPS];
  __local struct spn_subgroup_smem volatile * const smem = smem_wg + get_sub_group_id();
#endif

  //
  // this is a subgroup/warp-centric kernel
  //
  // which subgroup in the grid is this?
  //
  // TAKE NOTE: the Intel GEN compiler appears to be recognizing
  // get_group_id(0) as a uniform but the alternative calculation used
  // when there are multiple subgroups per workgroup is not
  // cooperating and driving spillage elsewhere.
  //
#if ( SPN_RASTERIZE_WORKGROUP_SUBGROUPS == 1 )
  uint const cmd_idx = get_group_id(0);
#else
  uint const cmd_idx = get_group_id(0) * SPN_RASTERIZE_WORKGROUP_SUBGROUPS + get_sub_group_id();
#endif

#if 0
  if (get_sub_group_local_id() == 0)
    printf("+cmd_idx = %u\n",cmd_idx);
#endif

  //
  // if worksgroups are multi-subgroup then there may be excess
  // subgroups in the final workgroup
  //
  if (cmd_idx >= count)
    return;

#if 0
  if (get_sub_group_local_id() == 0)
    printf("-cmd_idx = %u\n",cmd_idx);
#endif

  //
  // load a single command for this subgroup
  //
  union spn_cmd_rasterize const cmd = cmds[cmd_idx];

#if 0
  if (get_sub_group_local_id() == 0)
    printf("[ %u ]< %u, %u, %u, %u >\n",
           cmd_idx,
           cmd.nodeword,
           SPN_CMD_RASTERIZE_GET_TRANSFORM(cmd),
           SPN_CMD_RASTERIZE_GET_CLIP(cmd),
           SPN_CMD_RASTERIZE_GET_COHORT(cmd));
#endif

  //
  // get first block node command word and its subblock
  //
  spn_uint              nodeword = cmd.nodeword; // nodeword has word-addressing
  spn_tagged_block_id_t tag_id   = bp_blocks[nodeword].tag_id;
  spn_block_id_tag      tag      = SPN_TAGGED_BLOCK_ID_GET_TAG(tag_id);
  spn_block_id_t        id       = SPN_TAGGED_BLOCK_ID_GET_ID(tag_id);

  //
  // load transform -- uniform across subgroup
  //
  // v8: { sx shx tx shy sy ty w0 w1 }
  //
  // NOTE THAT WE'RE SCALING UP THE TRANSFORM BY:
  //
  //   [ SPN_SUBPIXEL_RESL_X_F32, SPN_SUBPIXEL_RESL_Y_F32, 1.0f ]
  //
  // Coordinates are scaled to subpixel resolution.  All that matters
  // is that continuity is maintained between end path element
  // endpoints.
  //
  // It's the responsibility of the host to ensure that the transforms
  // are properly scaled either via intitializing a transform stack
  // with the subpixel resolution scaled identity or scaling the
  // transform before its loaded by a rasterization grid.
  //
  // FIXME -- horizontal load might be better than this broadcast load
  //
  union spn_transform const tv     = { .f32v8 = transforms[SPN_CMD_RASTERIZE_GET_TRANSFORM(cmd)] }; // uniform load
  union spn_path_clip const cv     = { .f32v4 = clips     [SPN_CMD_RASTERIZE_GET_CLIP(cmd)     ] }; // uniform load
  spn_uint            const cohort = SPN_CMD_RASTERIZE_MASK_COHORT(cmd); // shifted

  switch (tag)
    {
    case SPN_BLOCK_ID_TAG_PATH_LINE:
      spn_rasterize_lines(bp_atomics,
                          bp_blocks,
                          bp_ids,
                          bp_mask,
                          cohort_atomics,
                          sk_extent,
                          smem,
                          &nodeword,&id,
                          &tv,&cv,cohort);
      break;

    case SPN_BLOCK_ID_TAG_PATH_QUAD:
      spn_rasterize_quads(bp_atomics,
                          bp_blocks,
                          bp_ids,
                          bp_mask,
                          cohort_atomics,
                          sk_extent,
                          smem,
                          &nodeword,&id,
                          &tv,&cv,cohort);
      break;

    case SPN_BLOCK_ID_TAG_PATH_CUBIC:
      spn_rasterize_cubics(bp_atomics,
                           bp_blocks,
                           bp_ids,
                           bp_mask,
                           cohort_atomics,
                           sk_extent,
                           smem,
                           &nodeword,&id,
                           &tv,&cv,cohort);
      break;

    case SPN_BLOCK_ID_TAG_PATH_RAT_QUAD:
      break;
    case SPN_BLOCK_ID_TAG_PATH_RAT_CUBIC:
      break;

    default:
      break;
    }
}

//
//
//

#endif
